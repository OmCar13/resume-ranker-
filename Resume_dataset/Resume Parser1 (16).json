{"content": "R N Prajapati\nB-44, DELTA-I, Greater Noida\nEmail: narain.prajapati@gmail.com\nPhone: (M) +91 9599732235\n•\nTotal 2+ years of Experience in Microsoft SQL SERVER/SSRS/SSIS\n· Currently associated with WEBIT EXPERT, Noida as a Database Developer\n· Experienced in programming tasks-Stored Procedures, function, and Cursors using SQL Server 2008 R2/2012 with T-SQL.\n\n· Expertise in generating reports using SQL Server Reporting Services (SSRS).\n· Expertise in generating reports using Cisco Unified Intelligence Center (CUIC).\n· Expertise in SQL Server Integration Services (SSIS)\n· Working out of client side in a dynamic and high-pressure environment.\n\n· Capable of working in team and as well as individually to meet timelines\nTechnical Proficiency\n\n· MS-SQL Server\n· SSIS\n· SSRS\n\n· CUIC\n\nEducational Credentials\n· MCA from Uttar Pradesh Technical University, Lucknow in 2009.\n\n· M.Sc. from VBS Purvanchal University Jaunpur in 2004\n· B.Sc. from VBS Purvanchal University Jaunpur in 2002.\nProfessional Experience\n   WEBIT EXPERT, NOIDA\n\n\n\n \n                               (Sept’2017–Till date)\n\nProjects Executed\n\tProject#1\n\tIRAS (Inland Revenue Authority of Singapore)\n\n\tTechnology used\n\tSQL Server 2012, SSRS, SSIS, CUIC, UCCE Schemas\n\n\tTeam size\n\t4\n\n\tRole\n\tDatabase Developer\n\n\tDuration\n\t18 Months\n\n\n\n\n\tDescription\n\tIRAS Omni-channel contact center is a project with Dimension data (NTT) in collaboration with CISCO, used to voice Call and email Survey report  Details Daily/monthly basis, this report shall show the survey response rating given to each Agent for each survey question according to the time of the survey\n\n\tResponsibilities\n\t· Process analysis and writing SQL Scripts\n· Proper documentation according of reports\n· Reports Development using SSRS/CUIC. \n\n· Weekly and Monthly Activities of integrating with external systems \n\n\n\tProject#2\n\tBNPP Reporting Solution\n\n\tTechnology used\n\tSQL Server 2012, SSRS, SSIS, UCCE Schemas\n\n\tTeam size\n\t3\n\n\tRole\n\tDatabase Developer\n\n\tDuration\n\t8 Months\n\n\n\n\n\tDescription\n\tThere are eight custom report created in CUIC for inbound, outbound traffic. This report is used to find out CSR inbound-outbound Calls, CSR Talk time Worktime Hold time, Waiting Time and Abandoned Calls\n\n\tResponsibilities\n\t· Process analysis \n\n· designing and writing SQL Scripts\n· Reports Development using CUIC. \n\n\n\tProject#3\n\tCollege Management ERP\n\n\tTechnology used\n\tSQL server 2008R2, SSRS, SSIS.\n\n\tDescription\n\tIt was a web based application for managing all the activities of a School or college such as: -\nA). Employee management: -\n Manages all the detail of employee such as employee registration, salary of employee etc.\nB). Payroll: - \n It is the most important part of software by which we can manage salary related records.\nC). Student Registration, Exam, Online Exam, Administration, Transport Facilities is the other    parts of this ERP.\n\n\tDuration\n\t5 Months\n\n\tResponsibilities\n\n\n\t· Designed, implemented and maintained.\n\n· Participating in Code review.\n· Software Documentation and unit testing\n\n\nPersonal Details\n\n  Name\n\n\n\n: R N Prajapati\n\n  Date of Birth\n\n\n: 05-06-1982\n\n  Nationality\n\n\n: Indian\n\n  Sex\n\n\n\n: Male\n\n  Language Known\n\n: English, Hindi\n\n\n1\nPage 1 of 2","annotation":[{"label":["Name"],"points":[{"start":3063,"end":3075,"text":"R N Prajapati"}]},{"label":["Skills"],"points":[{"start":2414,"end":2417,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":2408,"end":2411,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":2017,"end":2219,"text":"There are eight custom report created in CUIC for inbound, outbound traffic. This report is used to find out CSR inbound-outbound Calls, CSR Talk time Worktime Hold time, Waiting Time and Abandoned Calls"}]},{"label":["designation"],"points":[{"start":1959,"end":1976,"text":"Database Developer"}]},{"label":["Skills"],"points":[{"start":1917,"end":1920,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":1911,"end":1914,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":1894,"end":1908,"text":"SQL Server 2012"}]},{"label":["Skills"],"points":[{"start":1754,"end":1757,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":1314,"end":1617,"text":"IRAS Omni-channel contact center is a project with Dimension data (NTT) in collaboration with CISCO, used to voice Call and email Survey report  Details Daily/monthly basis, this report shall show the survey response rating given to each Agent for each survey question according to the time of the survey"}]},{"label":["designation"],"points":[{"start":1255,"end":1272,"text":"Database Developer"}]},{"label":["Skills"],"points":[{"start":1207,"end":1210,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":1201,"end":1204,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":1184,"end":1198,"text":"SQL Server 2012"}]},{"label":["Total experience"],"points":[{"start":930,"end":981,"text":"B.Sc. from VBS Purvanchal University Jaunpur in 2002"}]},{"label":["Total experience"],"points":[{"start":875,"end":926,"text":"M.Sc. from VBS Purvanchal University Jaunpur in 2004"}]},{"label":["Total experience"],"points":[{"start":810,"end":869,"text":"MCA from Uttar Pradesh Technical University, Lucknow in 2009"}]},{"label":["Skills"],"points":[{"start":770,"end":773,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":763,"end":766,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":747,"end":759,"text":"MS-SQL Server"}]},{"label":["Skills"],"points":[{"start":567,"end":570,"text":"SSIS"}]},{"label":["Total experience"],"points":[{"start":521,"end":570,"text":"Expertise in SQL Server Integration Services (SSIS"}]},{"label":["Total experience"],"points":[{"start":439,"end":516,"text":"Expertise in generating reports using Cisco Unified Intelligence Center (CUIC)"}]},{"label":["Skills"],"points":[{"start":430,"end":433,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":361,"end":434,"text":"Expertise in generating reports using SQL Server Reporting Services (SSRS)"}]},{"label":["designation"],"points":[{"start":221,"end":238,"text":"Database Developer"}]},{"label":["Experience in current company"],"points":[{"start":180,"end":214,"text":"associated with WEBIT EXPERT, Noida"}]},{"label":["Skills"],"points":[{"start":163,"end":166,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":158,"end":161,"text":"SSRS"}]},{"label":["Phone"],"points":[{"start":92,"end":101,"text":"9599732235"}]},{"label":["Email"],"points":[{"start":50,"end":75,"text":"narain.prajapati@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"R N Prajapati"}]}],"extras":null,"metadata":{"first_done_at":1624274943000,"last_updated_at":1624274943000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "RESUME\nATUL MISHRA\nContact     No.: 8130227859  Email Id:  at.mi82@yahoo.in\nCurrent location:-NOIDA.\nCareer Objective:\nLooking for a Middle position in IT and gradually move on to Senior Level position in the IT Company, which assures career growth in terms of knowledge and job satisfaction.\nExperience:\t\n1.Working at CRM Services India Pvt Ltd(A Subsidiary of Teleperformance USA) as Database Programmer since Jan  2015 .\n2. Worked at Denave India Private Limited as Database Engineer from  3rd of Feb 2014 to Jan 2015.\n3. Worked at NEXGEN CONSULTANCY Pvt Ltd Noida as a Database Developer from 16th Aug 2011 to 29th Jan 2014.\nTOTAL IT EXPERIENCE: 8 YEARS ON SQL SERVER(TSQL,SSRS,SSIS),MySQL.\nCurrent CTC: 7.4(625900+Night Shift Allowance(72000)+Conveyance(30000)+Internet(12000)) Lakhs per annum.\nTechnical Proficiency:\n1. I have good knowledge of  TSQL,stored procedures ,triggers and cursors.\n2.I  have also worked on BI tool like SSRS,SSIS.\n3.Have good knowledge of making parametrised ,drill  down,drill through  SSRS reports.\n4.I  am also an Oracle Certified Associate (OCA) in DBA track.\n5.Have knowledge of taking database backup,restoring databases ,  doing query optimization  and    other dba related works etc\n\nDomains:\n Working on Call Center domain at CRM Services India PVT Ltd(Teleperformance).\nHave worked on Sales and marketing domain at Denave India Pvt Ltd.                        \n Have worked on Rajasthan R-APDRP PROJECT (Power domain) at Nexgen Consultancy Pvt Ltd. \n\n\nJob Responsbilities:\nCompany: CRM Services India Pvt Ltd(Subsidiary of Teleperformance).\n1.Created stored Procedures,functions,complex sql queries using TSQL.\n2. Designed new reports and wrote technical documentation, gathered requirements, analyzed data, developed and built different types of SSRS reports(drill down,drill through,parameterised) and dashboard, supported many client companies' general business reports and benefit plan administration processes.\n3.Created SSIS packages for every new process .\nCompany: Denave India Pvt Ltd.\n1. Created stored Procedures,functions,complex sql queries using TSQL.\n2. Created different types of SSRS reports(Parameterised,Drill Down,Drill Through,Subreports,Dashboards).\n3. Created SSIS packages for every new process .\n4.Doing DBA related task like creating new databases,taking backup and restoring databases on new servers,creating linked servers etc.\n Company: Nexgen Consultancy Pvt Ltd.\n1. Created stored Procedures,functions,complex sql queries using TSQL.\n2. Created different types of SSRS reports(Parameterised,Drill Down,Drill Through,Subreports,Dashboards).\n3. Created SSIS packages for every new process .\n\nMy Roles In The Project:\n1. Doing Migration of Data using Tsql stored procedures,triggers and cursors.\n2. Taking backup,doing query optimization,creating complex stored procedure ,triggers and cursors.\n3. Also making complex MIS SSRS reports .\n4. Performing data migration using ETL tools like SSIS.\n5. Optimizing SQL queries, PL SQL & TSQL codes to return results in minimal time.\n6. Also to gather and transform business requirements into innovative BI solutions that effectively support business users.\n7. Document designs, specifications, and flow diagrams to assist in efficient development, support, and problem analysis.\n8. Hands-on development of dynamic dashboards, grids, graphs, scorecards, and statistical reporting utilizing BI Tools.\n9. Create and enhance report objects, metrics, filters, and prompts.\n10. Develop dashboards and reports per specified requirements and timelines.\n11. Participate in data model design and data build for the enterprise datawarehouse.\n12. Develop, execute, and document test plans for development activities.\n13. Validate reporting results against requirements, and troubleshoot and analyze data issues within reports.\n14. Develop a strong understanding of the source systems that pass data to the reporting systems, and the relationships within the data.\n15. Support and manage special projects, ad-hoc management reporting requests, testing, and issue resolution.\n16. Develop and maintain documentation for reporting and tracking all changes.\n17. Communicate clearly and effectively with business users and technical teams through all phases of the development lifecycle.\n     18. Meet regularly with internal departments and resources to review current    reporting and     identify improvements and efficiencies.\n\nEducational Details:\n· Bachelor of Engineering(Computer Science And Engineering) from RPSIT Magadh University Bodh Gaya .Passed in Oct 2010 with 72.83%\n\n· 10+2 (ICSE) from CISCE  in 2003 with 53%.\n\n· 10(ICSEE) from CISCE in 2001 with 64.5%.\n\nSoftwares:\nProgramming    : TSQL,SQL*PLUS ,SSRS,SSIS ,OCA CERTIFIED(DBA).\nOperating System      :   Vista/WIN 8\nORACLE ID                                          : OC0911608\nMARKS SCORED IN SQL                       : 98\nMARKS SCORED IN FUNDAMENTAL-1    : 96\n\n    Strengths:\n                Positive attitude, Optimistic, Eager to Learn and improve constantly,Sincerity.\nHobbies/Interests:\n                           Reading books,  Internet browsing. \nCo-Curricular:\n                  Participated in Technical Symposiums, seminars in various colleges.\n\nExtra-Curricular:.\n                           Long Jump , Cycling,Sketching.\n\n\n\nPersonal Details:\n\nDOB\t\t\t    :  May 31 , 1985\nSex\t\t\t    :  Male\nLanguages known\t    :  English, Hindi\nMarital Status\t                  :  Married   \nNationality                           :  Indian\nFather’s name                     :  SHIVENDRA KUMAR MISHRA\n\nPermanent Address                      :  S/O-Mr.SHIVENDRA KUMAR MISHRA\n                                                             VILL+PO-KOELWAR\n                                                             MOHALA-MISHRATOLA\n                                                             DIST-BHOJPUR\n                                                             STATE- BIHAR\n                                                              PIN-802160                                                             \n\nDeclaration: \n\nI hereby declare that the above-mentioned information is correct up to my knowledge and I bear the responsibility for the correctness of the above-mentioned particulars.\n                                                                                                                                                      (Atul Mishra,)","annotation":[{"label":["Skills"],"points":[{"start":4860,"end":4862,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4712,"end":4715,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":4702,"end":4704,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4698,"end":4700,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4697,"end":4700,"text":"TSQL"}]},{"label":["10 %"],"points":[{"start":4627,"end":4665,"text":"10(ICSEE) from CISCE in 2001 with 64.5%"}]},{"label":["12 %"],"points":[{"start":4582,"end":4621,"text":"10+2 (ICSE) from CISCE  in 2003 with 53%"}]},{"label":["Highest degree"],"points":[{"start":4450,"end":4577,"text":"Bachelor of Engineering(Computer Science And Engineering) from RPSIT Magadh University Bodh Gaya .Passed in Oct 2010 with 72.83%"}]},{"label":["Total experience"],"points":[{"start":3648,"end":3715,"text":"Develop, execute, and document test plans for development activities"}]},{"label":["Total experience"],"points":[{"start":3485,"end":3555,"text":"Develop dashboards and reports per specified requirements and timelines"}]},{"label":["Skills"],"points":[{"start":3001,"end":3003,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3000,"end":3003,"text":"TSQL"}]},{"label":["Skills"],"points":[{"start":2994,"end":2996,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2978,"end":2980,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2893,"end":2896,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":2770,"end":2863,"text":"Taking backup,doing query optimization,creating complex stored procedure ,triggers and cursors"}]},{"label":["Total experience"],"points":[{"start":2617,"end":2659,"text":"Created SSIS packages for every new process"}]},{"label":["Skills"],"points":[{"start":2538,"end":2541,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":2503,"end":2505,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2502,"end":2505,"text":"TSQL"}]},{"label":["Total experience"],"points":[{"start":2440,"end":2505,"text":"Created stored Procedures,functions,complex sql queries using TSQL"}]},{"label":["Total experience"],"points":[{"start":2218,"end":2260,"text":"Created SSIS packages for every new process"}]},{"label":["Skills"],"points":[{"start":2139,"end":2142,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":2104,"end":2106,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2103,"end":2106,"text":"TSQL"}]},{"label":["Total experience"],"points":[{"start":2041,"end":2106,"text":"Created stored Procedures,functions,complex sql queries using TSQL"}]},{"label":["Total experience"],"points":[{"start":1961,"end":2003,"text":"Created SSIS packages for every new process"}]},{"label":["Skills"],"points":[{"start":1790,"end":1793,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":1657,"end":1956,"text":"Designed new reports and wrote technical documentation, gathered requirements, analyzed data, developed and built different types of SSRS reports(drill down,drill through,parameterised) and dashboard, supported many client companies' general business reports and benefit plan administration processes"}]},{"label":["Skills"],"points":[{"start":1649,"end":1651,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1648,"end":1651,"text":"TSQL"}]},{"label":["Total experience"],"points":[{"start":1586,"end":1651,"text":"Created stored Procedures,functions,complex sql queries using TSQL"}]},{"label":["Experience in current company"],"points":[{"start":1525,"end":1550,"text":"CRM Services India Pvt Ltd"}]},{"label":["Total experience"],"points":[{"start":1099,"end":1158,"text":"Have knowledge of taking database backup,restoring databases"}]},{"label":["Total experience"],"points":[{"start":1036,"end":1094,"text":"I  am also an Oracle Certified Associate (OCA) in DBA track"}]},{"label":["Skills"],"points":[{"start":1020,"end":1023,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":936,"end":939,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":900,"end":944,"text":"I  have also worked on BI tool like SSRS,SSIS"}]},{"label":["Skills"],"points":[{"start":853,"end":855,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":852,"end":855,"text":"TSQL"}]},{"label":["Total experience"],"points":[{"start":826,"end":895,"text":"I have good knowledge of  TSQL,stored procedures ,triggers and cursors"}]},{"label":["Skills"],"points":[{"start":690,"end":692,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":677,"end":680,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":673,"end":675,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":672,"end":675,"text":"TSQL"}]},{"label":["Skills"],"points":[{"start":661,"end":663,"text":"SQL"}]},{"label":["Number of company worked"],"points":[{"start":535,"end":560,"text":"NEXGEN CONSULTANCY Pvt Ltd"}]},{"label":["Number of company worked"],"points":[{"start":437,"end":464,"text":"Denave India Private Limited"}]},{"label":["Experience in current company"],"points":[{"start":319,"end":344,"text":"CRM Services India Pvt Ltd"}]},{"label":["Total experience"],"points":[{"start":119,"end":290,"text":"Looking for a Middle position in IT and gradually move on to Senior Level position in the IT Company, which assures career growth in terms of knowledge and job satisfaction"}]},{"label":["Email"],"points":[{"start":59,"end":74,"text":"at.mi82@yahoo.in"}]},{"label":["Phone"],"points":[{"start":36,"end":45,"text":"8130227859"}]},{"label":["Name"],"points":[{"start":7,"end":17,"text":"ATUL MISHRA"}]}],"extras":null,"metadata":{"first_done_at":1624274693000,"last_updated_at":1624274693000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Zaid Akhtar  \nOracle DBA  \nzaidoracledba@gmail.com  \n\nContact: 7977853152  \n-------------------------------------------------------------------------------------------------------------------------------  \n\nProfile Summary:  \nAround 4+ years of experience as an Oracle DBA,and sql DBA providing 24X7 production and  \n\nNon-Production support for Oracle Database 11g/12c on different platforms like Red Hat Linux and \nWindows and AIX.  \n\nCurrently working as a Software Engineer with Servion Global Solution Ltd.  \n \n\nTechnical Skills:  \n• RDBMS: 11gR2, 12C  \n\n• Software Tools : SQL* PLUS, RMAN, DBCA, Netmgr,dbua,sqldevloper,SSIS,SSMS,OEM,SNOW \n\n• Operating Systems : Windows 8, RHEL-5/6/7 OEL-5/6 AIX, SunSol.  \n• Languages : SQL.  \n\n \nSpecialization:  \n• Managing Tablespaces, Data files, Control files, Redo log files and Archive logs.  \n• Creating, managing and monitoring database users and assigning them privileges.  \n\n• Setting RMAN configuration parameters.  \n\n• Incrementally updated RMAN backup.  \n• Cloning the Oracle database using RMAN  \n\n• Good knowledge in implementation and administration of dataguard.  \n\n• Implementation of snapshot standby database.  \n• Responsible for Logical Database Backups (EXPDP & IMPDP).  \n\n• Support high availability environment using Oracle RAC,Dataguard and RMAN.  \n\n• Worked on Oracle (ASM) with single Instance & RAC databases .  \n• Ensure disaster recovery (DR sites) for the production environment and regular  \n\n  DR drills are performed. \n• Performance Tunning .   \n\n• Patching.  \n\n• Basic knowledge on MySql database .  \n• Basic knowledge on Azure  and Mssql Database. \n\n• Basic knowledge on MaxDB. \n\n \n\nProfessional Experience Summary:  \nEmployer: Servion Global Solution Ltd.  \nClient: HDFC  (Mumbai)  \n\nDuration:March 2019 to Till now.  \n\nRole : Oracle DBA  \n\nResponsibilities:  \n\n• Creating, managing & monitoring Database Users and assigning them Privileges. \n•  Managing Control files, Redo Log files , Tablespaces, Datafiles, and Archive logs \n• Configured and Maintaining Database Backups:(RMAN)  \n\n• Monitoring daily backup & checking log files.  \n\n\n\n• Maintaining tablespaces resize the datafiles, adding a datafile to a tablespace.  \n\n• Reorganization (index rebuild & gather stats) of tables.  \n• Backup certain tables and Schemas using Datapump EXPDP/IMPDP .  \n\n• Monitoring Databases through OEM.  \n\n• Managing storage clauses while creating tablespaces \n• Checking alert log to know types of bottleneck recorded during the day. \n\n• Managing database auditing request and doing database hardening.  \n• Configured data migration through sql database to oracle database using SSIS tool.  \n\n• Monitor and manage daily basis data migration count.    \n\n• Creating store procedure to generating bulk Extract .  \n• Creating and managing jobs for regular task. \n\n• Manage performance of query and execution task of jobs.  \n\n• Generating code for bulk extract as per requested timestamp through multiple databases. \n• Managing UAT server for troubleshooting. \n\n \nEmployer: GapBridge Software service Pvt Ltd.  \n\nClient: Accenture (Mumbai)  \n\nDuration: Oct 2018 to Feb 2019.  \nRole : Oracle DBA \n\n• Managing users and their roles in the database.  \n\n• Planning and Maintaining Database Backups: Cold Backup, Hot Backup (RMAN) \n  & LogicalBackup.  \n\n• Monitoring daily hot backup & checking log file to ensure successful backup completion.  \n\n• Maintaining tablespaces resize the datafiles, adding a datafile to a tablespace.  \n• Datapump EXPDP/IMPDP for backing certain tables and Schemas.  \n\n• Taking regular backups both logical and physical on timely basis , Logical Backups  \n(Table level, Tablespace level, schema level and database level).  \n\n• Configured Dataguard & monitored physical standby database for high availability.  \n\n• Monitoring Databases through OEM.  \n• Logical backup & restore (mysqldump ) on mysql database.  \n\n• Configured Percona tool on Mysql Servers.  \n\n• Check status for MaxDB and Hana (SAP) backup.  \n• Ensuring snapshot backup status on Azure portal.  \n\n• Monitoring OMS tool and pull report for failure backup \n\n \n\nEmployer: Ample softech system Pvt Ltd.  \n\nClient: Nutrisynapzz (Mumbai)  \nDuration: May 2015 to Sept 2018  \n\nRole : Oracle DBA.  \n\n• Check daily checklist and send to respective DLs.  \n• Maintenance and monitoring 11gR2 Oracle RAC Databases and single instances .  \n\n• Managing users and their roles, user’s profile in the database.  \n\n• Maintaining Database Backups: Hot Backup (RMAN) & Logical Backup.  \n• Monitoring and Maintenance of physical standby database.  \n\n• Reorganization of tables and maintaining statistics.  \n• Monitoring daily hot backup & checking log file to ensure successful backup completion.  \n\n• Load or unload data between databases using Datapump utilities expdp and impdp.  \n\n• Performed planned DR-DRILLS (i.e. switchover and switchback) \n\n\n\n \n \n\nEducation:  \n• B-tech Galgotias college of Engineering & Technology (Greater Noida) 2014.  \n\n• H.S.C (B.S.E.B) with 64% in 2009.  \n\n• S.S.C (B.S.E.B) with 68% in 2007. \n\n \n\nPersonal Details:  \n• Name : Zaid Akhtar  \n\n• Date Of Birth : 20-11-1991  \n• Marital status : Single,  \n\n• Nationality : Indian  \n\n• Language : English, Hindi, Urdu  \n• Passport : Yes  \n\n \n\nPermanent Address                                                                              Local Address  \nBadi Jama masjid                                                                                   612- valmikinagar soceity \n\nWard no 22, Bhabua.                                                                             Tagornagar-5 vikroli \n\nKaimur Bihar -821101.                                                                                   Mumbai-400083","annotation":[{"label":["Name"],"points":[{"start":5087,"end":5097,"text":"Zaid Akhtar"}]},{"label":["10 %"],"points":[{"start":5019,"end":5050,"text":"S.S.C (B.S.E.B) with 68% in 2007"}]},{"label":["12 %"],"points":[{"start":4980,"end":5011,"text":"H.S.C (B.S.E.B) with 64% in 2009"}]},{"label":["Highest degree"],"points":[{"start":4900,"end":4972,"text":"B-tech Galgotias college of Engineering & Technology (Greater Noida) 2014"}]},{"label":["Skills"],"points":[{"start":4324,"end":4328,"text":"11gR2"}]},{"label":["designation"],"points":[{"start":4226,"end":4235,"text":"Oracle DBA"}]},{"label":["Number of company worked"],"points":[{"start":4120,"end":4146,"text":"mple softech system Pvt Ltd"}]},{"label":["Total experience"],"points":[{"start":3555,"end":3702,"text":"Taking regular backups both logical and physical on timely basis , Logical Backups  \n(Table level, Tablespace level, schema level and database level"}]},{"label":["Total experience"],"points":[{"start":3212,"end":3303,"text":"Planning and Maintaining Database Backups: Cold Backup, Hot Backup (RMAN) \n  & LogicalBackup"}]},{"label":["designation"],"points":[{"start":3144,"end":3153,"text":"Oracle DBA"}]},{"label":["Total experience"],"points":[{"start":2888,"end":2973,"text":"Generating code for bulk extract as per requested timestamp through multiple databases"}]},{"label":["Skills"],"points":[{"start":2644,"end":2647,"text":"SSIS"}]},{"label":["Total experience"],"points":[{"start":2572,"end":2652,"text":"Configured data migration through sql database to oracle database using SSIS tool"}]},{"label":["Total experience"],"points":[{"start":2372,"end":2422,"text":"Managing storage clauses while creating tablespaces"}]},{"label":["Total experience"],"points":[{"start":2009,"end":2057,"text":"Configured and Maintaining Database Backups:(RMAN"}]},{"label":["Total experience"],"points":[{"start":1842,"end":1917,"text":"Creating, managing & monitoring Database Users and assigning them Privileges"}]},{"label":["designation"],"points":[{"start":1805,"end":1814,"text":"Oracle DBA"}]},{"label":["Experience in current company"],"points":[{"start":1705,"end":1731,"text":"Servion Global Solution Ltd"}]},{"label":["Total experience"],"points":[{"start":1539,"end":1571,"text":"Basic knowledge on MySql database"}]},{"label":["Total experience"],"points":[{"start":1238,"end":1310,"text":"Support high availability environment using Oracle RAC,Dataguard and RMAN"}]},{"label":["Skills"],"points":[{"start":727,"end":729,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":630,"end":633,"text":"SSMS"}]},{"label":["Skills"],"points":[{"start":625,"end":628,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":595,"end":598,"text":"DBCA"}]},{"label":["Skills"],"points":[{"start":578,"end":586,"text":"SQL* PLUS"}]},{"label":["Skills"],"points":[{"start":578,"end":580,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":545,"end":549,"text":"11gR2"}]},{"label":["Skills"],"points":[{"start":538,"end":542,"text":"RDBMS"}]},{"label":["Experience in current company"],"points":[{"start":482,"end":508,"text":"Servion Global Solution Ltd"}]},{"label":["Total experience"],"points":[{"start":318,"end":430,"text":"Non-Production support for Oracle Database 11g/12c on different platforms like Red Hat Linux and \nWindows and AIX"}]},{"label":["designation"],"points":[{"start":262,"end":271,"text":"Oracle DBA"}]},{"label":["Phone"],"points":[{"start":63,"end":72,"text":"7977853152"}]},{"label":["Email"],"points":[{"start":27,"end":49,"text":"zaidoracledba@gmail.com"}]},{"label":["designation"],"points":[{"start":14,"end":23,"text":"Oracle DBA"}]},{"label":["Name"],"points":[{"start":0,"end":10,"text":"Zaid Akhtar"}]}],"extras":null,"metadata":{"first_done_at":1624274843000,"last_updated_at":1624274843000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Resume\n\nName: Jyoti Sharma                  \t\t\t\t\tAddress: D1-102, Jasminium Society\nE-mail: jyots.1994.sharma@gmail.com\t\t\t\tMagarpatta City, \nContact Number: 7599355883       \t\t\t\tPune, Maharashtra\n\t\t\t\t\t\t\t\tPincode: 411028\n\t\t\t\t\t\t\t\t\t\t\t________\n\tCareer Objective\n\n\t\nSeeking a challenging & growth-oriented career with a progressive organization that could utilize my skills at greater extent while being resourceful, innovative and flexible.\n\n\n\n\n\tWork Experience\n\n\t\nVirtusa Polaris: December 2016 - Present\n\n\n\n\n\tProfessional Profile\n\n\t\n· Overall 2.6 years of work Experience as Software Engineer in VirtusaPolaris. \n· Involved in Unix Shell Scripting, PL/SQL code development and SQL for database queries. \n· Expertise in understanding the requirements and verifying the same.\n· Proficient in analyzing the root cause of the issues on different environments.\n· Skilled in writing shell scripts in UNIX.\n· Strong analytical skills for identifying functional and technical requirements.\n· Excellent interpersonal, communication and analytical skills.\n\n\n\n\tEducational details\n\n\tQualification\n\n\tCollege/School\n\tBoard/University\n\tPercentage\n\tYear of Passing\n\n\tB.Tech (Computer Science & Engg.)\n\tNoida Institute of Engineering and Technology, Greater Noida\n\tDr. A. P. J Abdul Kalam Technical University\n\t81.22 %\n(aggregate)\n\t2016\n\n\tS.S.E\n\tWisdom Public School, Aligarh\n\tCBSE\n\t78%\n\t2012\n\n\tH.S.E\n\tWisdom Public School, Aligarh\n\tCBSE\n\t80%\n\t2010\n\n\n\n\n\n\n\n\tProject Details\n\n\tProjects\n\t1. TRIMS (Trade Resource Information Management System) – Development \n2. Credit Risk – Development and Pre-Prod Support. \n\n\tDomain\n\tBanking \n\n\tClient\n\tCITI \n\n\tEnvironment\n\tUnix, Windows\n\n\tDescription\n\tTRIMS manage trade between customers and Citi bank. System capture details and transaction related to customers from frontend and process the gathered information to provide it to Partner Systems. It used to maintain all the trading information of CITI Bank. \n\nCredit Risk creates the simulation for all the trades performed in CITI over a duration of 15 years. It basically finds out the Risk Exposure related to trades done with Citi Bank. Also provides support for all the UAT environments. Right from basic setup of PROD like code, Creating and executing Test Cases, Monitoring before and after cases. \n\n\tLanguages\n\tUnix Shell Scripting, PL/SQL and SQL. \n\n\tDuration\n\tTRIMS – 1.2 years.\nCredit Risk – 1.4 year.\n\n\tRoles & Responsibilities\n\t\n· Determine Production issues and building appropriate code for addressing the issue.\n· Testing the transaction data for a customer. \n· Deploying DML’s to maintain data consistency.\n· Working on Go-Lives and Rollouts of different Regions. \n· Handling new business requirements.\n· Preparing BRD and FRD for any requirement. \n· Testing a software used by users/clients and determine if it can be accepted or not.\n· Performing functional, system and regression Testing.\n· Calculating the risk of all the trades over a duration of 15 years.\n· Performs setups on multiple environments to generate the exposure reports.\n· Creation of multiple test cases based on JIRA.\n· Developing scripts for automation related to tedious flow.\n· Monitoring the performance level and prioritize the sessions accordingly.\n· Extraction and maintain production code in UAT repository.\n· Interact with DEV team for all the bugs and defects and preparation of test cases accordingly.\n· Deploying code using GitHub.\n\n\n\n\n\n\n\\\n\n\tAchievements\n\n\t\n· Multiple Recognitions and Appreciation from Clients and Managers. \n· Won 3rd Prize in Poster making competition conducted by NIET (2014).\n· Won 2nd Prize in Rangoli Competition conducted by NIET (2014).\n· Certified for Singing at State level by Bharat Vikas Praisad (2008).\n· Won all-rounder award in school. \n\n\n\n\n\n\tCertifications\n\n\t\n· Certification in the field of Big Data Analysis from Iota Cell (2015).\n· Microsoft Certified student in the field of Software Development (2015).\n· Certification in the field of Core Java from Aptech, Aligarh (2014).\n\n\n\n\n\n\tCompetencies\n\n\t\n· Good grasper and result oriented.\n· Enjoy learning new technologies.\n· Excellent interpersonal, communication and analytical skills.\n· Dedicated and Creative.\n· Capability to work in a team-oriented as well as independent environments.\n\n\n\n\n\n\tPersonnel Snippets\n\n\t\nPermanent Address\t                    : H.no – 1/36 Sanjay Gandhi Colony (Ravanteela),\n                                                         Aligarh, 202001\n\nDate of Birth                                   : 09/11/1994\n\nNationality\t: Indian\n\nLanguages\t: English, Hindi. \n\t\nHobbies\t: Singing, Cooking, Dancing and Solving Logical puzzles.\n\nMarital Status                                : Single\n\n\n\n\n\tDeclaration\n\n\t\nI hereby declare that all information given above is correct to the best of my knowledge and belief.\n\n\n\nPlace: Pune                                                                 \t\t\t\nDate: \t14 Feb, 2019\t\t\t\t\t\t             Jyoti Sharma","annotation":[{"label":["Name"],"points":[{"start":4909,"end":4920,"text":"Jyoti Sharma"}]},{"label":["Skills"],"points":[{"start":2322,"end":2324,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2314,"end":2316,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2311,"end":2316,"text":"PL/SQL"}]},{"label":["Total experience"],"points":[{"start":1669,"end":1925,"text":"TRIMS manage trade between customers and Citi bank. System capture details and transaction related to customers from frontend and process the gathered information to provide it to Partner Systems. It used to maintain all the trading information of CITI Bank"}]},{"label":["10 %"],"points":[{"start":1384,"end":1412,"text":"Wisdom Public School, Aligarh"}]},{"label":["12 %"],"points":[{"start":1328,"end":1356,"text":"Wisdom Public School, Aligarh"}]},{"label":["12 %"],"points":[{"start":1321,"end":1325,"text":"S.S.E"}]},{"label":["Highest degree"],"points":[{"start":1185,"end":1244,"text":"Noida Institute of Engineering and Technology, Greater Noida"}]},{"label":["Highest degree"],"points":[{"start":1150,"end":1181,"text":"B.Tech (Computer Science & Engg."}]},{"label":["Total experience"],"points":[{"start":900,"end":977,"text":"Strong analytical skills for identifying functional and technical requirements"}]},{"label":["Total experience"],"points":[{"start":774,"end":851,"text":"Proficient in analyzing the root cause of the issues on different environments"}]},{"label":["Total experience"],"points":[{"start":704,"end":769,"text":"Expertise in understanding the requirements and verifying the same"}]},{"label":["Skills"],"points":[{"start":675,"end":677,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":650,"end":652,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":647,"end":652,"text":"PL/SQL"}]},{"label":["Total experience"],"points":[{"start":613,"end":698,"text":"Involved in Unix Shell Scripting, PL/SQL code development and SQL for database queries"}]},{"label":["Total experience"],"points":[{"start":533,"end":607,"text":"Overall 2.6 years of work Experience as Software Engineer in VirtusaPolaris"}]},{"label":["Phone"],"points":[{"start":157,"end":166,"text":"7599355883"}]},{"label":["Email"],"points":[{"start":92,"end":118,"text":"jyots.1994.sharma@gmail.com"}]},{"label":["Name"],"points":[{"start":14,"end":25,"text":"Jyoti Sharma"}]}],"extras":null,"metadata":{"first_done_at":1624273872000,"last_updated_at":1624273872000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Resume\n\nName: Jyoti Sharma                  \t\t\t\t\tAddress: D1-102, Jasminium Society\nE-mail: jyots.1994.sharma@gmail.com\t\t\t\tMagarpatta City, \nContact Number: 7599355883       \t\t\t\tPune, Maharashtra\n\t\t\t\t\t\t\t\tPincode: 411028\n\t\t\t\t\t\t\t\t\t\t\t________\n\tCareer Objective\n\n\t\nSeeking a challenging & growth-oriented career with a progressive organization that could utilize my skills at greater extent while being resourceful, innovative and flexible.\n\n\n\n\n\tWork Experience\n\n\t\nVirtusa Polaris: December 2016 - Present\n\n\n\n\n\tProfessional Profile\n\n\t\n· Overall 3 years of work Experience as Software Engineer in VirtusaPolaris. \n· Involved in Unix Shell Scripting, PL/SQL code development and SQL for database queries. \n· Expertise in understanding the requirements and verifying the same.\n· Proficient in analyzing the root cause of the issues on different environments.\n· Skilled in writing shell scripts in UNIX.\n· Strong analytical skills for identifying functional and technical requirements.\n· Excellent interpersonal, communication and analytical skills.\n\n\n\n\tEducational details\n\n\tQualification\n\n\tCollege/School\n\tBoard/University\n\tPercentage\n\tYear of Passing\n\n\tB.Tech (Computer Science & Engg.)\n\tNoida Institute of Engineering and Technology, Greater Noida\n\tDr. A. P. J Abdul Kalam Technical University\n\t81.22 %\n(aggregate)\n\t2016\n\n\tS.S.E\n\tWisdom Public School, Aligarh\n\tCBSE\n\t78%\n\t2012\n\n\tH.S.E\n\tWisdom Public School, Aligarh\n\tCBSE\n\t80%\n\t2010\n\n\n\n\n\n\n\n\tProject Details\n\n\tProjects\n\t1. TRIMS (Trade Resource Information Management System) – Development \n2. Credit Risk – Development and Pre-Prod Support. \n\n\tDomain\n\tBanking \n\n\tClient\n\tCITI \n\n\tEnvironment\n\tUnix, Windows\n\n\tDescription\n\tTRIMS manage trade between customers and Citi bank. System capture details and transaction related to customers from frontend and process the gathered information to provide it to Partner Systems. It used to maintain all the trading information of CITI Bank. \n\nCredit Risk creates the simulation for all the trades performed in CITI over a duration of 15 years. It basically finds out the Risk Exposure related to trades done with Citi Bank. Also provides support for all the UAT environments. Right from basic setup of PROD like code, Creating and executing Test Cases, Monitoring before and after cases. \n\n\tLanguages\n\tUnix, Shell Scripting, PL/SQL, SQL and C++. \n\n\tDuration\n\tTRIMS – 1.5 years.\nCredit Risk – 1.5 year.\n\n\tRoles & Responsibilities\n\t\n· Determine Production issues and building appropriate code for addressing the issue.\n· Testing the transaction data for a customer. \n· Deploying DML’s to maintain data consistency.\n· Working on Go-Lives and Rollouts of different Regions. \n· Handling new business requirements.\n· Preparing BRD and FRD for any requirement. \n· Testing a software used by users/clients and determine if it can be accepted or not.\n· Performing functional, system and regression Testing.\n· Calculating the risk of all the trades over a duration of 15 years.\n· Performs setups on multiple environments to generate the exposure reports.\n· Creation of multiple test cases based on JIRA.\n· Developing scripts for automation related to tedious flow.\n· Monitoring the performance level and prioritize the sessions accordingly.\n· Extraction and maintain production code in UAT repository.\n· Interact with DEV team for all the bugs and defects and preparation of test cases accordingly.\n· Deploying code using GitHub.\n\n\n\n\n\n\n\\\n\n\tAchievements\n\n\t\n· Multiple Recognitions and Appreciation from Clients and Managers. \n· Won 3rd Prize in Poster making competition conducted by NIET (2014).\n· Won 2nd Prize in Rangoli Competition conducted by NIET (2014).\n· Certified for Singing at State level by Bharat Vikas Praisad (2008).\n· Won all-rounder award in school. \n\n\n\n\n\n\tCertifications\n\n\t\n· Certification in the field of Big Data Analysis from Iota Cell (2015).\n· Microsoft Certified student in the field of Software Development (2015).\n· Certification in the field of Core Java from Aptech, Aligarh (2014).\n\n\n\n\n\n\tCompetencies\n\n\t\n· Good grasper and result oriented.\n· Enjoy learning new technologies.\n· Excellent interpersonal, communication and analytical skills.\n· Dedicated and Creative.\n· Capability to work in a team-oriented as well as independent environments.\n\n\n\n\n\n\tPersonnel Snippets\n\n\t\nPermanent Address\t                    : H.no – 1/36 Sanjay Gandhi Colony (Ravanteela),\n                                                         Aligarh, 202001\n\nDate of Birth                                   : 09/11/1994\n\nNationality\t: Indian\n\nLanguages\t: English, Hindi. \n\t\nHobbies\t: Singing, Cooking, Dancing and Solving Logical puzzles.\n\nMarital Status                                : Single\n\n\n\n\n\tDeclaration\n\n\t\nI hereby declare that all information given above is correct to the best of my knowledge and belief.\n\n\n\nPlace: Pune                                                                 \t\t\t\nDate: \t14 Feb, 2019\t\t\t\t\t\t             Jyoti Sharma","annotation":[{"label":["Name"],"points":[{"start":4913,"end":4924,"text":"Jyoti Sharma"}]},{"label":["Total experience"],"points":[{"start":3433,"end":3496,"text":"Multiple Recognitions and Appreciation from Clients and Managers"}]},{"label":["10 %"],"points":[{"start":1382,"end":1410,"text":"Wisdom Public School, Aligarh"}]},{"label":["12 %"],"points":[{"start":1326,"end":1354,"text":"Wisdom Public School, Aligarh"}]},{"label":["Highest degree"],"points":[{"start":1183,"end":1242,"text":"Noida Institute of Engineering and Technology, Greater Noida"}]},{"label":["Highest degree"],"points":[{"start":1148,"end":1179,"text":"B.Tech (Computer Science & Engg."}]},{"label":["Total experience"],"points":[{"start":772,"end":849,"text":"Proficient in analyzing the root cause of the issues on different environments"}]},{"label":["Total experience"],"points":[{"start":702,"end":767,"text":"Expertise in understanding the requirements and verifying the same"}]},{"label":["Total experience"],"points":[{"start":611,"end":696,"text":"Involved in Unix Shell Scripting, PL/SQL code development and SQL for database queries"}]},{"label":["Total experience"],"points":[{"start":533,"end":605,"text":"Overall 3 years of work Experience as Software Engineer in VirtusaPolaris"}]},{"label":["Total experience"],"points":[{"start":261,"end":434,"text":"Seeking a challenging & growth-oriented career with a progressive organization that could utilize my skills at greater extent while being resourceful, innovative and flexible"}]},{"label":["Phone"],"points":[{"start":157,"end":166,"text":"7599355883"}]},{"label":["Email"],"points":[{"start":92,"end":118,"text":"jyots.1994.sharma@gmail.com"}]},{"label":["Name"],"points":[{"start":14,"end":25,"text":"Jyoti Sharma"}]}],"extras":null,"metadata":{"first_done_at":1624275031000,"last_updated_at":1624275031000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Vandana Mittal                          \nE-mail:   vandanarock94@gmail.com\n\nMob. No.: +918448970093, 9997068793\n\nCareer Experience \n\nTo obtain a position as a developer utilizing my education, experience, and communication skills to improve the company’s efficiency by providing exceptional IT services.\n\n\nProfessional Experience\n\n· Having Around 1 Year 10month’s professional experience as a Developments & Application support. \n· Having 1 year 10 months of professional software development experience in the areas of Microsoft Business Intelligence (MSBI) and Having Knowledge in Data Warehousing (DW) applications primarily using Microsoft SQL Server. \n· Having 1.5 years of knowledge about BI application support solving complex problems ,performance, availably, provide correct solution for live application   \n· Having 2 months of professional dashboard development experience in the areas of Microsoft Power BI and Power BI Desktop. Primarily using Microsoft SQL Server. \n· Strong process/analytical knowledge of MS-BI Developer and Data Analyst in Production, Development and Staging Environments.\n· Good Communication skills, committed team player, creative, self-starter, having ability to work in a fast-paced environment\n\n\nExperience Summary\n· Current Employer: Optum Global Solution PVT LTD.\nDesignation:  Associate Management System Analyst (April -2017 to till date). \nRole & Job Responsibility: \n· Working in Extract, Transform & Load (ETL) development using SQL Server Integration Services (SSIS).\n· Working in Hands on experience in SQL Server Reporting Service (SSRS) .\n· Working in Thorough Knowledge on Data Warehouse concepts like Star Schema, Snow Flake, Dimension and Fact Tables.\n· Working in Implementing and maintaining existing systems. Fixing some of legacy bugs which are client specific (CR’s).\n· Working in Hands of experience in creating Jobs, Alerts, SQL Mail Agent, Database Mail and Scheduled DTS and SSIS Packages.\n· Working in auditing and processing platform (APP Tool) using SQL query  to update ,delete and modify the table as per client requirement.\n· Creations of database objects like tables, Functions, view, join and stored procedure using SQL Server.\n· Worked in UCS Omega Project as a backend developer using SQL Server \n\n\n\n\nProject name\n\n1. Title: UHCG Inventory\n      Tool: Microsoft SQL Server, SSIS, And SSRS Role: Developer\nDescription: UHCG inventory store the data of united health care group. In this project we have to create the date warehouse for UHCG inventory. For this we have write lot of SSIS packages that move data from different source (excel, flat files, SQL server) to DWH tables. Same time also working for data visualization through SSRS.\n\n1. Title: Automated Reports\n      Tool: Microsoft SQL Server, SSIS, And SSRS Role: Developer\nDescription: Automated various excel reports by import the data in database and Create view , stored procedure and deploy the report in SSRS .\n\n1. Title: UCS Omega\n      Tool: Microsoft SQL Server Role: Developer\nDescription: UCS omega acquires data from the various data sources of the company and insert or update data into Data warehouse .We have to create stored procedure, views, function and temp table as per the client requirement\n\n1. Title: Auditing and Processing Platform(APP tool)\n      Tool: Microsoft SQL Server Role: Developer\nDescription: It is web – based application. Working as a database support .Delete and update the Customer record as  per client specification.\n\nTechnical Skill Set:\nOperating System   :   Windows family\nLanguages               :   SQL SERVER, T SQL,C\nDevelopment Tools:   Visual Studio 2010/2008, \nScript Language      :   VB Script. \nETL Tool                  : SQL Server Integration Services (SSIS),\nReporting Tool         : SQL Server Reporting Service (SSRS), Power BI \n\nProfessional Qualification\n\n· Bachelor of Technology (B.Tech), (2012-2016), through Meerut Institute of Technology, Meerut. (UPTU  Lucknow) \n\nAcademic Qualification\n\n· Intermediate from C.B.S.E Board in 2010. \n· High School from C.B.S.E Board in 2012.\n\nPERSONAL PROFILE:\n\nLanguage known\t:  English, Hindi (read, write, speak).\nMarital status\t\t:  Single\nGender \t\t\t: Female\n\n\tSUMMARY\n\n\n\nI have a well background of computer knowledge and good knowledge of data communication and networking. apart from good technical & logical skill, good communication skill, attitude to work in a group and zeal towards quick learning with sincerity & commitment are some of my positives.\n\nPlace..............                                                                         \t\t\nDate...............\t\t\t\t\t\t\t\t\t\t   (Vandana Mittal)","annotation":[],"extras":null,"metadata":{"first_done_at":1624274032000,"last_updated_at":1624274032000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Vandana Mittal                          \nE-mail:   vandanarock94@gmail.com\n\nMob. No.: +918448970093, 9997068793\n\nCareer Experience \n\nTo obtain a position as a developer utilizing my education, experience, and communication skills to improve the company’s efficiency by providing exceptional IT services.\n\n\nProfessional Experience\n\n· Having Around 2 Year 6 month’s professional experience as a Developments & Application support. \n· Having 2 year 3 months of professional software development experience in the areas of Microsoft Business Intelligence (MSBI) and Having Knowledge in Data Warehousing (DW) applications primarily using Microsoft SQL Server. \n· Having 1.5 years of knowledge about BI application support solving complex problems ,performance, availably, provide correct solution for live application   \n· Having  5 months of professional dashboard development experience in the areas of Microsoft SQL Server Reporting Services. Primarily using Microsoft SQL Server. \n· Strong process/analytical knowledge of MS-BI Developer and Data Analyst in Production, Development and Staging Environments.\n· Good Communication skills, committed team player, creative, self-starter, having ability to work in a fast-paced environment\n\n\nExperience Summary\n· Current Employer: R1RCM PVT LTD\n\nDesignation: Engineer –BI and Analytics (May -2019 to till date). \nRole & Job Responsibility: \n· Working in Extract, Transform & Load (ETL) development using SQL Server Integration Services (SSIS).\n· Working in Implementing and maintaining existing systems. Fixing some of legacy bugs which are client specific (CR’s).\n· Working in Hands of experience in creating Jobs, Alerts, SQL Mail Agent, Database Mail and Scheduled DTS and SSIS Packages.\n· Working in auditing and processing platform (R! Tool) using SQL query  to update ,delete and modify the table and create SSIS package  as per client requirement.\n· Creations of database objects like tables, Functions, view, join and stored procedure using SQL Server.\n\n\nProject name\n\n1. Title: ICD and HCPCS\n      Tool: Microsoft SQL Server and SSIS,  Role: Developer\nDescription: HCPCS and ICD data sent by client in excel then create table and dumb the data in table through SSIS then transfer the data in different table through procedure as per client requirement.  \n\n1. Title: Historical data Load in Different Tables\n      Tool: Microsoft SQL Server and SSIS Role: Developer\nDescription: Load the historical data which is not in table  of retro,payments and memo as per reqirements.\n\n·  Optum Global Solution PVT LTD.\nDesignation:  Associate Management System Analyst (April -2017 to May-2019). \nRole & Job Responsibility: \n· Working in Extract, Transform & Load (ETL) development using SQL Server Integration Services (SSIS).\n· Working in Hands on experience in SQL Server Reporting Service (SSRS) .\n· Working in Thorough Knowledge on Data Warehouse concepts like Star Schema, Snow Flake, Dimension and Fact Tables.\n· Working in Implementing and maintaining existing systems. Fixing some of legacy bugs which are client specific (CR’s).\n· Working in Hands of experience in creating Jobs, Alerts, SQL Mail Agent, Database Mail and Scheduled DTS and SSIS Packages.\n· Working in auditing and processing platform (APP Tool) using SQL query  to update ,delete and modify the table as per client requirement.\n· Creations of database objects like tables, Functions, view, join and stored procedure using SQL Server.\n· Worked in UCS Omega Project as a backend developer using SQL Server \n\n\nProject name\n\n1. Title: UHCG Inventory\\\n      Tool: Microsoft SQL Server, SSIS, And SSRS Role: Developer\nDescription: UHCG inventory store the data of united health care group. In this project we have to create the date warehouse for UHCG inventory. For this we have write lot of SSIS packages that move data from different source (excel, flat files, SQL server) to DWH tables. Same time also working for data visualization through SSRS.\n\n2. Title: Automated Reports\n      Tool: Microsoft SQL Server, SSIS, And SSRS Role: Developer\nDescription: Automated various excel reports by import the data in database and Create view , stored procedure and deploy the report in SSRS .\n\n3. Title: UCS Omega\n      Tool: Microsoft SQL Server Role: Developer\nDescription: UCS omega acquires data from the various data sources of the company and insert or update data into Data warehouse .We have to create stored procedure, views, function and temp table as per the client requirement\n\n4. Title: Auditing and Processing Platform(APP tool)\n      Tool: Microsoft SQL Server Role: Developer\nDescription: It is web – based application. Working as a database support .Delete and update the Customer record as  per client specification.\n\nTechnical Skill Set:\nOperating System   :   Windows family\nLanguages               :   SQL SERVER, T SQL,C\nDevelopment Tools:   Visual Studio 2010/2008, \nScript Language      :   VB Script. \nETL Tool                  : SQL Server Integration Services (SSIS),\nReporting Tool         : SQL Server Reporting Service (SSRS), \n\nProfessional Qualification\n\n· Bachelor of Technology (B.Tech), (2012-2016), through Meerut Institute of Technology, Meerut. (UPTU  Lucknow) \n\nAcademic Qualification\n\n· Intermediate from C.B.S.E Board in 2010. \n· High School from C.B.S.E Board in 2012.\n\nPERSONAL PROFILE:\n\nLanguage known\t:  English, Hindi (read, write, speak).\nMarital status\t\t:  Single\nGender \t\t\t: Female\n\n\tSUMMARY\n\n\n\nI have a well background of computer knowledge and good knowledge of data communication and networking. apart from good technical & logical skill, good communication skill, attitude to work in a group and zeal towards quick learning with sincerity & commitment are some of my positives.\n\nPlace..............                                                                         \t\t\nDate...............\t\t\t\t\t\t\t\t\t\t   (Vandana Mittal)","annotation":[{"label":["Name"],"points":[{"start":5868,"end":5881,"text":"Vandana Mittal"}]},{"label":["Skills"],"points":[{"start":5296,"end":5296,"text":"C"}]},{"label":["10 %"],"points":[{"start":5279,"end":5316,"text":"High School from C.B.S.E Board in 2012"}]},{"label":["Skills"],"points":[{"start":5253,"end":5253,"text":"C"}]},{"label":["12 %"],"points":[{"start":5235,"end":5273,"text":"Intermediate from C.B.S.E Board in 2010"}]},{"label":["Highest degree"],"points":[{"start":5097,"end":5205,"text":"Bachelor of Technology (B.Tech), (2012-2016), through Meerut Institute of Technology, Meerut. (UPTU  Lucknow)"}]},{"label":["Skills"],"points":[{"start":5028,"end":5037,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":4996,"end":4999,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":4963,"end":4972,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":4923,"end":4931,"text":"VB Script"}]},{"label":["Skills"],"points":[{"start":4849,"end":4849,"text":"C"}]},{"label":["Skills"],"points":[{"start":4843,"end":4847,"text":"T SQL"}]},{"label":["Skills"],"points":[{"start":4831,"end":4840,"text":"SQL SERVER"}]},{"label":["Skills"],"points":[{"start":4697,"end":4697,"text":"C"}]},{"label":["Skills"],"points":[{"start":4573,"end":4582,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":4285,"end":4285,"text":"C"}]},{"label":["Skills"],"points":[{"start":4244,"end":4253,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":4213,"end":4213,"text":"C"}]},{"label":["Skills"],"points":[{"start":4138,"end":4138,"text":"C"}]},{"label":["Skills"],"points":[{"start":4027,"end":4030,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":4015,"end":4024,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3806,"end":3809,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":3762,"end":3762,"text":"C"}]},{"label":["Skills"],"points":[{"start":3646,"end":3646,"text":"C"}]},{"label":["Skills"],"points":[{"start":3600,"end":3603,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":3588,"end":3597,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3552,"end":3552,"text":"C"}]},{"label":["Skills"],"points":[{"start":3512,"end":3521,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3466,"end":3466,"text":"C"}]},{"label":["Skills"],"points":[{"start":3441,"end":3450,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3349,"end":3349,"text":"C"}]},{"label":["Skills"],"points":[{"start":3192,"end":3195,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":3074,"end":3074,"text":"C"}]},{"label":["Total experience"],"points":[{"start":2961,"end":3204,"text":" Working in Implementing and maintaining existing systems. Fixing some of legacy bugs which are client specific (CR’s).\n· Working in Hands of experience in creating Jobs, Alerts, SQL Mail Agent, Database Mail and Scheduled DTS and SSIS Packages"}]},{"label":["Skills"],"points":[{"start":2806,"end":2815,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":2763,"end":2766,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":2730,"end":2739,"text":"SQL Server"}]},{"label":["Total experience"],"points":[{"start":2669,"end":2767,"text":"Working in Extract, Transform & Load (ETL) development using SQL Server Integration Services (SSIS)"}]},{"label":["Skills"],"points":[{"start":2397,"end":2400,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":2382,"end":2391,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":2214,"end":2217,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":2129,"end":2129,"text":"C"}]},{"label":["Skills"],"points":[{"start":2121,"end":2121,"text":"C"}]},{"label":["Skills"],"points":[{"start":2119,"end":2119,"text":"C"}]},{"label":["Total experience"],"points":[{"start":2118,"end":2303,"text":"HCPCS and ICD data sent by client in excel then create table and dumb the data in table through SSIS then transfer the data in different table through procedure as per client requirement"}]},{"label":["Skills"],"points":[{"start":2082,"end":2085,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":2067,"end":2076,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":2042,"end":2042,"text":"C"}]},{"label":["Skills"],"points":[{"start":2040,"end":2040,"text":"C"}]},{"label":["Skills"],"points":[{"start":2032,"end":2032,"text":"C"}]},{"label":["Skills"],"points":[{"start":1993,"end":2002,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":1901,"end":1901,"text":"C"}]},{"label":["Skills"],"points":[{"start":1858,"end":1861,"text":"SSIS"}]},{"label":["Total experience"],"points":[{"start":1737,"end":1896,"text":"Working in auditing and processing platform (R! Tool) using SQL query  to update ,delete and modify the table and create SSIS package  as per client requirement"}]},{"label":["Skills"],"points":[{"start":1720,"end":1723,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":1602,"end":1602,"text":"C"}]},{"label":["Total experience"],"points":[{"start":1489,"end":1732,"text":" Working in Implementing and maintaining existing systems. Fixing some of legacy bugs which are client specific (CR’s).\n· Working in Hands of experience in creating Jobs, Alerts, SQL Mail Agent, Database Mail and Scheduled DTS and SSIS Packages"}]},{"label":["Skills"],"points":[{"start":1481,"end":1484,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":1448,"end":1457,"text":"SQL Server"}]},{"label":["Total experience"],"points":[{"start":1387,"end":1485,"text":"Working in Extract, Transform & Load (ETL) development using SQL Server Integration Services (SSIS)"}]},{"label":["Skills"],"points":[{"start":1278,"end":1278,"text":"C"}]},{"label":["Experience in current company"],"points":[{"start":1275,"end":1287,"text":"R1RCM PVT LTD"}]},{"label":["Skills"],"points":[{"start":1257,"end":1257,"text":"C"}]},{"label":["Skills"],"points":[{"start":1114,"end":1114,"text":"C"}]},{"label":["Total experience"],"points":[{"start":982,"end":1104,"text":"Strong process/analytical knowledge of MS-BI Developer and Data Analyst in Production, Development and Staging Environments"}]},{"label":["Skills"],"points":[{"start":967,"end":976,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":910,"end":919,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":643,"end":652,"text":"SQL Server"}]},{"label":["Total experience"],"points":[{"start":333,"end":426,"text":"Having Around 2 Year 6 month’s professional experience as a Developments & Application support"}]},{"label":["Skills"],"points":[{"start":113,"end":113,"text":"C"}]},{"label":["Phone"],"points":[{"start":101,"end":110,"text":"9997068793"}]},{"label":["Phone"],"points":[{"start":89,"end":98,"text":"8448970093"}]},{"label":["Email"],"points":[{"start":51,"end":73,"text":"vandanarock94@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":13,"text":"Vandana Mittal"}]}],"extras":null,"metadata":{"first_done_at":1624274003000,"last_updated_at":1624274003000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "M Madhukar \n\nRole     :      Data Steward(INFORMATION STEWARD) & BODS Developer\nContact: +91 9705280258\nEmail   :     madhukars1k91@gmail.compm\n                                                                                                                                                                                                 PROFESSIONAL SUMMARY:\n\n· Having around 4 years of experience in Data Warehousing. This includes analysis and development of Dataflows, mappings using needed transformations using BODS.\n· Developing Jobs, Workflows and Data flows based on the specifications with using various stages in BODS.\n· Developing Mappings based on the specifications with using various stages in BODS.\n· Worked with SAP Functional team build validation rules on area with the data anomalies.\n· Built validation rules and scorecard to help functional team to identify the trend on data fixes.\n· Worked on impact and Lineage analysis by building metadata integrator on various sources.\n· Built jobs for address cleansing.\n· Worked on building glossary of terms on Matapedia.\n· Working experience on Profiling, writing Data Quality Rules, developing Score cards and Match Review using Information Steward.\n· Built profiling tasks on source tables on Information Steward.\n· Worked to create business rules in “Data Insight” module of SAP information steward and these rules were quality check rules for business users\n· Extensively worked on BODS Designer Components-Projects, Jobs, Workflow, Data Flow, Scripts, Data Stores and Formats.\n· Knowledge on Data Quality Transformation.\n· Implemented SCD type2 dimensions to maintain the History of dimensional data.\n· Good knowledge on Data warehousing and Database concepts.\n· Exposure in BODS performance tuning techniques using parallel processing (Degree of Parallelism), Multithreading, Partitioning, and Database Throughputs to improve job performance.\n· Extensively used Data Services Management Console for Monitoring, Execution and Scheduling BODS jobs.\n· Experience in debugging execution errors using Data Services logs (trace, statics and Error) and by examining the target data.\n· Knowledge on CMC, creating the users and assign the rights to the users.\n\nWORK EXPERIENCE SUMMARY:\n\n· Worked as ETL Developer for “LIKEWAYS INFOCOM PRIVATE LIMITED”, Bangalore from Aug 2015 to Oct 2019.\n\n\nPROJECT DETAILS:\n\nClient:  Cardinal Health, Dublin, OH                                                                        \nRole:   BODS Developer\nDuration:  Jan 2017 to Oct 2019\nEnvironment:   Business Objects Data Services 4.2.\nDescription:  \nCardinal Health has built the industry's broadest suite of products\nand services that help improve quality, safety and efficiency all\nalong the chain of care. The purpose of the project is to develop an\nopen Enrolment process for different health plans. The data comes\nfrom different locations is collected and integrated into the data\nwarehouse.\n \nResponsibilities:\n\n· Developed the Dimensional Model for creating the Data ware house in Operational Data Source (ODS) environment. \n· Extracted the Source data through SAP BODS, an ETL tool to ODS target Dimensional Database, Handled the extraction of data in the way of Full Load and Incremental Data method by using Control tables\n· Created a Star and Snow-Flake Data Dimensional Schemas to achieve business needs.\n· Created complex Jobs, Work Flows, Data Flows, and Scripts using various Transforms () to successfully load data from source into a desired target.\n· Created the Data Quality rules in BOIS to measure the quality of the data within and across different systems or applications.\n\n· Created the Dim and Facts tables like Claim, Claimant, Body Part, Nature of Injury, Line of Insurance, Payment and Reserves to achieve the reporting needs. Used different ETL Transformations to filter and clean of quality of data and achieved the performance of ETL Jobs throughput of Optimization techniques\n· Implemented SCD type2 dimensions to maintain the History of dimensional data.\n· Achieved the granularity level as Day wise and handled the high level Aggregation in ETL to optimize the report performance\n· Formed matrix for basic column level profiling rules. Also non basic profiling as well like Uniqueness, Address, etc.\n· Responsible of SAP Information Steward administration using CMC.\n· Responsible of SAP DataServices administration using CMC and SAP BODS Management Console.\n· Involved in BODS Administration activities as daily support of Jobs and server monitoring\n\n\n\n\n\nClient:  POWERWAVE TECHNOLOGIES\nRole:  BODS Developer\nDuration:  Feb 2016 to Jan 2017\nEnvironment:  SAP BODS 4.1\n\nDescription:  \nThis Project is mainly to meet the reporting requirements of power wave technologies. A Wave technology is one of the leading companies which provide end to end solutions to the wireless network communications. It trades nearly with 75 countries around the globe.\n\nResponsibilities:\n\n· Involved in the development and implementation of the Enterprise Data Warehousing (EDW) process and Data Warehouse.\n· Building of the BODS Jobs, Workflows, Data Flows as per the Mapping Specification.\n· Worked extensively on different types of transformations like Query transformation, Merge, Case, Validations, Map-operation, History Preserving and Table Comparison transformations etc….\n· Extensively used ETL to load data from flat file and also from the relation database.\n· Extracted data from different sources such as Flat files , Oracle  to load into SQL database  \n· Ensuring proper Dependencies and Proper running of loads (Incremental and Complete loads) \n· Maintained warehouse metadata, naming standards and warehouse standards for future application development. \n· Involved in preparation and execution of the unit, integration and end to end test cases.\n\n\n\n\n\n\nClient\t\t:  A.P. Mark fed                                         \nDuration\t:  Aug 2015 to Jan 2016\nRole\t\t:  Software Developer\nEnvironment\t:   SAP BODS 4.0.\n\nDescription: \n\nIt is a federation of Marketing Co-operative Societies in A.P. With the main object of helping the farmers to secure better price for their produce by taking care of their market needs and providing agricultural inputs. Against this objective the Mark fed’s present activity consists of sale of farm inputs like chemical fertilizers, pesticides & seeds, maintenance of Godowns & procurement of Agricultural commodities through its member societies.\n\n\n\nResponsibilities:\n\n· Data Analysis on the Excel and Flat files to create the data base tables for loading the ICS clients data into the database.\n· Extracted the files data using multiple transformations to filtering the quality of data and loaded to Oracle db using the Data Integrator ETL \n· Understanding the Absence and Disability Systems to analyze the Disability Insurance claims data for developing the data model \n· Involved into oracle reporting to support the business users reporting needs.\n· Did the Feed Automation process for the ETL Jobs to run automatically and also scheduled the Jobs using Data Integrator administrator.","annotation":[{"label":["Total experience"],"points":[{"start":6005,"end":6451,"text":"It is a federation of Marketing Co-operative Societies in A.P. With the main object of helping the farmers to secure better price for their produce by taking care of their market needs and providing agricultural inputs. Against this objective the Mark fed’s present activity consists of sale of farm inputs like chemical fertilizers, pesticides & seeds, maintenance of Godowns & procurement of Agricultural commodities through its member societies"}]},{"label":["Total experience"],"points":[{"start":4669,"end":4930,"text":"This Project is mainly to meet the reporting requirements of power wave technologies. A Wave technology is one of the leading companies which provide end to end solutions to the wireless network communications. It trades nearly with 75 countries around the globe"}]},{"label":["Total experience"],"points":[{"start":4286,"end":4348,"text":"Responsible of SAP Information Steward administration using CMC"}]},{"label":["Total experience"],"points":[{"start":3960,"end":4035,"text":"Implemented SCD type2 dimensions to maintain the History of dimensional data"}]},{"label":["Total experience"],"points":[{"start":3649,"end":3956,"text":"Created the Dim and Facts tables like Claim, Claimant, Body Part, Nature of Injury, Line of Insurance, Payment and Reserves to achieve the reporting needs. Used different ETL Transformations to filter and clean of quality of data and achieved the performance of ETL Jobs throughput of Optimization techniques"}]},{"label":["Total experience"],"points":[{"start":3370,"end":3514,"text":"Created complex Jobs, Work Flows, Data Flows, and Scripts using various Transforms () to successfully load data from source into a desired target"}]},{"label":["Total experience"],"points":[{"start":3286,"end":3365,"text":"Created a Star and Snow-Flake Data Dimensional Schemas to achieve business needs"}]},{"label":["Total experience"],"points":[{"start":3085,"end":3282,"text":"Extracted the Source data through SAP BODS, an ETL tool to ODS target Dimensional Database, Handled the extraction of data in the way of Full Load and Incremental Data method by using Control tables"}]},{"label":["Total experience"],"points":[{"start":2971,"end":3079,"text":"Developed the Dimensional Model for creating the Data ware house in Operational Data Source (ODS) environment"}]},{"label":["Total experience"],"points":[{"start":2601,"end":2945,"text":"Cardinal Health has built the industry's broadest suite of products\nand services that help improve quality, safety and efficiency all\nalong the chain of care. The purpose of the project is to develop an\nopen Enrolment process for different health plans. The data comes\nfrom different locations is collected and integrated into the data\nwarehouse"}]},{"label":["Experience in current company"],"points":[{"start":2279,"end":2310,"text":"LIKEWAYS INFOCOM PRIVATE LIMITED"}]},{"label":["designation"],"points":[{"start":2260,"end":2272,"text":"ETL Developer"}]},{"label":["Total experience"],"points":[{"start":1592,"end":1667,"text":"Implemented SCD type2 dimensions to maintain the History of dimensional data"}]},{"label":["Total experience"],"points":[{"start":1428,"end":1543,"text":"Extensively worked on BODS Designer Components-Projects, Jobs, Workflow, Data Flow, Scripts, Data Stores and Formats"}]},{"label":["Total experience"],"points":[{"start":1087,"end":1212,"text":"Working experience on Profiling, writing Data Quality Rules, developing Score cards and Match Review using Information Steward"}]},{"label":["Total experience"],"points":[{"start":906,"end":993,"text":"Worked on impact and Lineage analysis by building metadata integrator on various sources"}]},{"label":["Total experience"],"points":[{"start":806,"end":901,"text":"Built validation rules and scorecard to help functional team to identify the trend on data fixes"}]},{"label":["Total experience"],"points":[{"start":716,"end":801,"text":"Worked with SAP Functional team build validation rules on area with the data anomalies"}]},{"label":["Total experience"],"points":[{"start":631,"end":711,"text":"Developing Mappings based on the specifications with using various stages in BODS"}]},{"label":["Total experience"],"points":[{"start":524,"end":626,"text":"Developing Jobs, Workflows and Data flows based on the specifications with using various stages in BODS"}]},{"label":["Total experience"],"points":[{"start":362,"end":519,"text":"Having around 4 years of experience in Data Warehousing. This includes analysis and development of Dataflows, mappings using needed transformations using BODS"}]},{"label":["Email"],"points":[{"start":118,"end":140,"text":"madhukars1k91@gmail.com"}]},{"label":["Phone"],"points":[{"start":93,"end":102,"text":"9705280258"}]},{"label":["Name"],"points":[{"start":0,"end":9,"text":"M Madhukar"}]}],"extras":null,"metadata":{"first_done_at":1624270174000,"last_updated_at":1624270174000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Suneel Kumar Reddy\n\tE-mail  : Suneel.nuka@gmail.com\nMobile  : 91-7989077199\n\n\n\n\nCarrier Goal \nSeeking the position of a MSBI developer to contribute my techincal knowledge on T-Sql, SSIS and SSRS help in organizational growth and development.\n\nProfessional Summary\n· Having 4.2 years of Experience in SQL Server 2014 and Microsoft business intelligence tools like (SSIS & SSRS ).\n· Extensive experience in dealing with Relational Database Management Systems including Normalization, Stored Procedures, Constraints, Querying, Joins, Keys, Indexes, Data Import/Export.\n· Proficient in designing and scheduling complex SSIS Packages for transferring data from multiple data sources to SQL Server.\n· Experienced in providing Logging, Error Handling by using Event Handler for SSIS Packages\n· Experience in creating Jobs, Alerts, SQL Mail Agent, and schedule SSIS Packages. \n· Experienced in designing and developing SSRS reports using data from ETL Loads, SSAS Cubes and various heterogeneous data sources.\n· Developed various types of complex reports like Drill Down, Drill Through, Matrix reports, Tabular reports.\n· Experienced in creating Ad-Hoc reports as per the client requirements.\n· Involved in scheduling, creating subscriptions for the reports using SSRS.\n·  Strong knowledge of Data Warehousing methodologies and concepts, including Star Schema and Snow Flake schemas\n· Ability to create documentations and permanent root cause analysis and corrective action plans.\n· Excellent analytical, communication and interpersonal skills. Proficient in technical writing and presentation and a good team player.\n· Ability to work independently or as part of a team to accomplish critical business objectives and to make decisions under pressure.\n\n\n\n\n\nTechnical Skills\n\n· ETL Tools                            : SQL Server Integration Services\n·  Reporting Tools               : SQL Server Reporting Services\n· Other Tools                        : MS-Office,  MSSQL \n· Modeling Technologies   : Jira and TFS\n· Operating System             : Windows Family    \n\n\n\n\n\n\nEducational Qualifications\t\t\t\n\n·   B. Tech from Jawaharlal Nehru Technological University.\n\nProfessional Experience\n\n· Working as a ETL Developer in NAVIGANT BPM INDIA PVT LTD from Sept’2016 to till date.\n· Worked as a Software Engineer in KPIT Global Solutions Pvt. Ltd from July’2015 to Aug’2016.\n\nProject Experience\n\n\n\nProject#1\n\n\tClient      :  Athens & Tri-Health\nProject   :  Comprehensive Care for Joint Replacement(CCJR)\nDomain   : Healthcare\n\tClient Location: U S\n\n\n\tTools        : SQL server 2014,SSIS \nDuration : Oct- 2018 – Till date\n\t\n\n\tTeam Size  : 4\nOnsite        :  2    \n\t\n\n\tProject Overview\n\tCJR model holds participant hospitals financially accountable for the quality and cost of a CJR episode of care and incentivizes increased coordination of care among hospitals, physicians, and post-acute care providers. The episode of care begins with an admission to a participant hospital of a beneficiary who is ultimately discharged under MS-DRG 469 (Major joint replacement or reattachment of lower extremity with major complications or comorbidities) or 470 (Major joint replacement or reattachment of lower extremity without major complications or comorbidities) and ends 90 days’ post-discharge in order to cover the complete period of recovery for beneficiaries. The episode includes all related items and services paid under Medicare Part A and Part B for all Medicare fee-for-service beneficiaries, except for certain exclusions.\n\n\n\n\tResponsibilities\n\t· Created new database objects like Procedures, Functions, Packages, Normalization, Indexes and Views using SQL in Development and Production environment for SQL Server.\n· Extracting the data from various Source systems which include various flat files in different formats (.txt, .CSV, excel, raw) and Loading into SQL Tables.\n· Designed packages Using Control Flow Tasks like For Loop Container,\n For Each Loop   Container, Sequential Container, Execute SQL Task and \nData Flow Task.\n· Extracted data from sources and transformed the data using \ndifferent transformations like data conversion, derived columns, look up, \nConditional Split, Aggregate, Union all, merge join and \nmulti cast transformations. \n· Created Event Handlers for the Packages Using Event Handler Tab for \nError Handling.\n· Maintained log information in SQL table to tracking Errors and \nrecording package execution status.\n· Worked on Jobs Scheduling and Monitoring Jobs in SQL Server Agent.\n· Created mapping using Designer Tool as per the Mapping specification document provided by the client.\n\n\tTechnologies:\n\n\t· MSBI Stack: SQL SERVER, SSIS\n· DBMS: SQL Server 2014, Visual studio 2013\n· Operating System – Windows 10\n\n\n\n\n\nProject#2\n\n\tClient      : Spartanburg, Loma Linda University Medical centre (LLUMC), Novant, Baptist Health Services(BHS)\nProject    : Bundle Payments for Care Improvement(BPCI)\nDomain    : Healthcare\n\tClient Location: U S\n\n\n\tTools        : SQL server 2014,SSIS \nDuration : Aug 2017 – Sept 2018\n\t\n\n\tTeam Size : 5\nOnsite        :  2    \n\t\n\n\tProject Overview\n\tThis product is related to health care domain which evolves the policy holders, members of the policy, member designees, providers, Insurance companies and Business units who visit their health care centers and hospitals.\n\n                  Patients visit the hospitals and physicians render services based on the patient’s health issues. Health care providers gather the patient’s insurance information and will incorporate in UB form and HCFA 1500 form format. These forms contain details like patient admission date, discharge date and all the health information of the insured patient. Once the patient receives the services from the health care centers or hospitals, these health care centers and hospitals claim the payments from the respective insurance companies providing the bills in the form of electronic bills or physical bills. Now the claims received by the insurance companies are validated to make the payments to the health care centers and hospitals. Payments are made after the review of the bills claimed. \n\n\n\tResponsibilities\n\t· Extensively worked on Creating and debugging stored procedures.\n· Created sql jobs to automate the process.\n· Used stored procedures to load the data into data warehouse, Configuration, Logging.\n· Creating packages on SSIS by using different data transformations like Conditional split, lookup, Derived column, Merge join& Sort to load data into database \n· Scheduling SSIS packages trough sql server agent jobs as per the daily and weekly basis.\n· Used report builder-2016 to create the reports.\n· Worked on various reports like tables, sub reports & Parameterized and drill trough reports in SSRS.\n· Produced various monthly, Quarterly reports by different type of reports using SSRS.\n· Created standard and data driven subscriptions as per the needs.\n· Generated Sub reports Drill down reports, Drill through reports, parameterized reports and linked reports from queries in SSRS as per the Client Requirements.\n· Deployed the reports into the report server and Created subscriptions as per client request.\n\n\n\tTechnologies:\n\n\t· MSBI Stack: SQL SERVER, SSIS, SSRS \n· DBMS: SQL Server 2012, Visual studio 2013, Report Builder\n· Operating System – Windows7\n\n\n\n\n\n\n\n\n\nProject#3\n\n\tClient            : Jubilant Food Works Limited\nProject         :  Jubilant BI Enterprise\n\n\tClient Location: INDIA\n\n\n\tTools            :  SQL server 2014,SSIS \nDuration     : July 2015 – Aug 2016\n\t\n\n\tTeam Size   : 6\n\t\n\n\tProject Overview\n\tJubilant Food Works Ltd is an Indian company based in Noida which owns the exclusive franchise rights for Dunkin Donuts branches in India and Domino's Pizza branches in the India, Nepal, Sri Lanka and Bangladesh. The company is a part of the Jubilant Bhartia Group.  It is the fastest growing Domino’s pizza company in the world. This Project is involving in creation and loading of Staging Database and Data warehouse with the required data which is gathered from several entities and then load into staging data base and then into Data Warehouse. The main object of the project is to analyse the Inventory for all warehouses through Cognos reports. Which helps them in decision making and to improve the business.\n\n\n\n\tResponsibilities\n\t· Involved in developing ETL for loading data into Database.\n· Creating Packages on SSIS by using different data Transformations   like Derived Column, Conditional Split to load data into Data Warehouse.\n· Involved in extracting data for cleansing activity from SQL source then transformed and loaded into Data Warehouse Targets using SSIS.\n· Worked on different tabs to develop complex packages like Control Flow, Data Flow, and Event Handlers etc.\n\n\n\t\n\t· Involved Creating Sql jobs to automate SSIS packages at schedule time.\n\n\n\tTechnologies:\n\n\t· MSBI Stack: SQL SERVER, SSIS \n· DBMS: SQL Server 2014, Visual studio 2013\n· Operating System – Windows7\n\n\n\n\t\n\nI hereby certify that the above information is true and correct to the best of my knowledge and belief.\n\n\n              \t\t\t\t\t\t Suneel Kumar Reddy","annotation":[{"label":["Name"],"points":[{"start":9086,"end":9103,"text":"Suneel Kumar Reddy"}]},{"label":["Skills"],"points":[{"start":8887,"end":8896,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":8873,"end":8876,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":8861,"end":8870,"text":"SQL SERVER"}]},{"label":["Skills"],"points":[{"start":8796,"end":8799,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":8635,"end":8638,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":8384,"end":8387,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":7478,"end":7481,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":7462,"end":7476,"text":"SQL server 2014"}]},{"label":["Skills"],"points":[{"start":7221,"end":7230,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":7207,"end":7210,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":7201,"end":7204,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":7189,"end":7198,"text":"SQL SERVER"}]},{"label":["Skills"],"points":[{"start":7024,"end":7027,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":6827,"end":6830,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":6740,"end":6743,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":6515,"end":6518,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":6364,"end":6367,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":4994,"end":4997,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":4978,"end":4992,"text":"SQL server 2014"}]},{"label":["Skills"],"points":[{"start":4664,"end":4673,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":4651,"end":4654,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":4639,"end":4648,"text":"SQL SERVER"}]},{"label":["Skills"],"points":[{"start":4485,"end":4494,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3694,"end":3703,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":2570,"end":2573,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":2554,"end":2568,"text":"SQL server 2014"}]},{"label":["Last company"],"points":[{"start":2303,"end":2332,"text":"KPIT Global Solutions Pvt. Ltd"}]},{"label":["Experience in current company"],"points":[{"start":2212,"end":2237,"text":"NAVIGANT BPM INDIA PVT LTD"}]},{"label":["Highest degree"],"points":[{"start":2098,"end":2151,"text":"B. Tech from Jawaharlal Nehru Technological University"}]},{"label":["Skills"],"points":[{"start":1957,"end":1961,"text":"MSSQL"}]},{"label":["Skills"],"points":[{"start":1945,"end":1953,"text":"MS-Office"}]},{"label":["Skills"],"points":[{"start":1876,"end":1885,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":1809,"end":1818,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":1257,"end":1260,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":1005,"end":1110,"text":"Developed various types of complex reports like Drill Down, Drill Through, Matrix reports, Tabular reports"}]},{"label":["Skills"],"points":[{"start":912,"end":915,"text":"SSRS"}]},{"label":["Total experience"],"points":[{"start":872,"end":1000,"text":"Experienced in designing and developing SSRS reports using data from ETL Loads, SSAS Cubes and various heterogeneous data sources"}]},{"label":["Skills"],"points":[{"start":854,"end":857,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":772,"end":775,"text":"SSIS"}]},{"label":["Total experience"],"points":[{"start":696,"end":784,"text":"Experienced in providing Logging, Error Handling by using Event Handler for SSIS Packages"}]},{"label":["Skills"],"points":[{"start":682,"end":691,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":616,"end":619,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":372,"end":375,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":365,"end":368,"text":"SSIS"}]},{"label":["Skills"],"points":[{"start":301,"end":310,"text":"SQL Server"}]},{"label":["Total experience"],"points":[{"start":267,"end":564,"text":"Having 4.2 years of Experience in SQL Server 2014 and Microsoft business intelligence tools like (SSIS & SSRS ).\n· Extensive experience in dealing with Relational Database Management Systems including Normalization, Stored Procedures, Constraints, Querying, Joins, Keys, Indexes, Data Import/Export"}]},{"label":["Skills"],"points":[{"start":191,"end":194,"text":"SSRS"}]},{"label":["Skills"],"points":[{"start":182,"end":185,"text":"SSIS"}]},{"label":["Phone"],"points":[{"start":65,"end":74,"text":"7989077199"}]},{"label":["Email"],"points":[{"start":30,"end":50,"text":"Suneel.nuka@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":17,"text":"Suneel Kumar Reddy"}]}],"extras":null,"metadata":{"first_done_at":1624271019000,"last_updated_at":1624271019000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "VEERENDRA DURGA RAM                    \n\n\n\nE-mail: alladaveerendra@gmail.com\n                                                                      \n\n\n     Mobile: +91 7780188827   __________________________________________________________________________                                                  \nObjective:\n        To use my skills and talent to learn new things and be a key part in the development of the organization besides gaining practical exposure of various components.\nProfile Summary:\n· Having 2 Years of working experience in Informatica Development.\n\n· Expertise in ETL Development Process using Informatica and SAP Data Services.\n\n· Extensively worked on data extraction, transformation and loading data from various sources like Teradata, Hadoop, MS SQL, Flat files.\n· Proficient in Dimensional Data Modelling Concepts-Star and Snowflake schema design.\n\n· Hands on experience in tuning mappings, identifying and resolving performance bottlenecks in various levels like sources, targets, mappings and sessions.\n\n· Having 2 years of working experience in SAP BO.\n· Expertise in creating the SAP BO universe using IDT.\n\n· Expertise in creating SAP BO WEBI reports.\n\n· Having 2 years of experience in Tableau Desktop.\n\n· Expertise in creating different dashboard and Story’s using Tableau Desktop. \n· Expertise in using all the utilities of Teradata.   \n\n· Good Knowledge in UNIX.\n\n· Excellent problem-solving skills with strong technical background and good interpersonal skills. Quick learner and excellent team player, ability to meet deadlines and work under pressure\n\n· Strong Communication, Self-motive and an attitude to learn the new technologies.\nSkill Set:\n\n· Languages\n\n\n\n\n\n:SQL,CSS, HTML.\n\n· Data base   \n\n\n\n\n\n: TERADATA V.15, MS SQL SERVER, ORACLE  11g(Basics), Hadoop.\n\n· ETL TOOL         \n\n\n\n: INFORMATICA 9.6.1-10.1.1., \n                                                SAP DATA SERVICES 4.2(Not real time)\n· Reporting Tool        \n\n\n: TABLEAU, SAP BOBJ 4.X\n\nSkill Set Summary:\n\nINFORMATICA:\n\n· Knowledge in developing various Mappings with the collection of all Sources, Targets, and transformations using INFORMATICA Designer.\n\n· Knowledge in creating repositories. \n\n· Extract and transform data from multiple sources such as SQL Server, Teradata, flat file, XML etc., and load into Teradata, Hadoop, MS SQL server tables which were used for reporting and sales force analytics.\n\n· Knowledge in designing Incremental loading process to load data into staging tables.\n\n· Developed Mappings using Transformations like Source Qualifier, Expression, Aggregator, Connected & unconnected lookups, Filter, Router, Rank, Sequence Generator, Sorter, Joiner, Stored Procedure, Normalizer and Router transformations and Update Strategy, XML Parser, SQL.\n\n· Implementing   Type 1 and Type 2 methodologies in ODS tables loading to keep historical data in data warehouse.\n\n· Debugged, edited break points while checking for data flow in transformations.\n\n· Creating and scheduling Sessions and Batch Process based on demand, run on time, or run only once using Informatica Workflow Manager and monitoring the data loads using the Workflow Monitor.\n\n· Create, design and implement the core ETL methodologies using Star schema.\n\n· Conducted knowledge transfer sessions with client resources and documented Data Mappings/ Transformations as per the business requirements.\n\n· Knowledge in data profiling and data validation using tools like Data analysis and Developing tool.\n\nTeradata:\n\n· Expertise in DDL, DML, TCL in Teradata database.\n· Expertise in tables, views, joins and hash indexes in Teradata database.\n\n· Extensively worked with Teradata utilities like BTEQ, Fast Export, Fast Load, Multi Load to export and load data to/from different source systems including flat files.\n\n· Expertise in writing large/complex queries using SQL.\n\n· Proficient in performance analysis, monitoring and SQL query tuning using EXPLAIN PLAN, Collect Statistics, Hints and SQL Trace.\n\n· Excellent Experience with different indexes (UPI, PI, SI, JI, AJIs, PPI (MLPPI, SLPPI)) and Collect Statistics.\n\n· Extensively worked with Teradata utilities like BTEQ, Fast Export, Fast Load, Multi Load to export\n\n   and load data to/from different source systems including flat files.\n· Very good in Analytical thinking, decision making and problem-solving skills.\nTableau:\n· Developed new Tableau visualizations based on the business need using different data sources like MS SQL, MS Excel, Hadoop.\n· Prepared and reviewed dashboard specifications and design documents.\n\n· Perform source data analysis, data discovery, and data profiling.\n\n· Experience in conducting performance tuning to ensure that dashboards function within acceptable parameters.\n\n· Developed and maintained Tableau dashboards, distribute, run and publish reports.\n\n· Developed ad hoc reports and complex dashboards in Tableau.\n\n· Big-data analysis using Tableau is a plus.\n\n· Experience in building dashboards using objects like Text tables, Box plots, whisker plots, heat maps, spider charts, tree maps, bullet charts, control charts, trellis charts, waterfall charts, Line, Dual Line chat, Ternary charts, Radial charts, Kaplan-Meier curve, bubble charts, symbol map, area charts in Tableau.\n\n· Solid experience of data visualization fundamentals.\n\n· 2 years of experience in Tableau with a strong experience in writing SQL queries.\n\n· Performed testing and fixed bugs.\nSAP BOBJ:\n\n· Expertise in Universe Building, Classes, Objects, Filters and LOVs at Universe Level for Business Analysis and Report Generation.\n\n· Good experience in creating and modifying Universes on Relational and OLAP Data sources.\n\n· Experience in building reports using Custom prompts, filters and conditions and testing the reports as per business requirements. \n· Knowledge on resolving join problems like loops by using alias, context.\n· Good Knowledge about dimensional model (Star Schema and Snowflake Schema).\n· Expertise on Design Derived tables on Universe level. \n· Expertise in working with WebI Intelligence (WebI).\n· Worked on Webi functionalities like break, section, sort, drill, charts, filters, alerts.\n· Import and export universes and reports from the repository and Checked Universe for integrity and exported to repository to make it available for user groups and Created users, user’s groups and maintained user access rights by using CMC.\n· Involved in support.\n· Knowledge on SQL (Structure Query Language).\n\nSAP Data Services (BODS):\n\n· Good Knowledge in data integrator transforms, platform transforms, Data Quality and text data processing transforms.\n\n· Knowledge in creating data stores and fetching data using data stores and file formats.\n\n· Knowledge in creating and using functions and custom functions and writing scripts. \n\n· Good knowledge in Data services 4.2 and worked on different Transport method, Shared directory, RFC and custom transfer to import the data from SAP ERP System.\n\n· Knowledge in creation of Repositories and migrating the batch jobs from local repo to local repo, multi using Environment.\n· Expertise in publishing the batch jobs to Central Repository and knowledge in data profiling using profile repository.\n· Good in creation of jobs using Business Objects Data Integrator & Data services.\n\n· Good Knowledge in migrating the data bases through work bench.\n· knowledge with Repository manager, Job server, Admin console, Metadata Integrator, Central Management Console.\n· Knowledge in migrating data from source to target and importing and exporting projects as .atl files.\n\n· Data cleansing and profiling data in Data Services.\n· Ability to create Workflows using object library and tool palette to execute data flows.\n\n· Knowledge on ERP data, BW data and FLAT FILE loading into HANA DB.\nOracle & MS SQL Server:\n\n· Knowledge on SQL/PL-SQL, T_SQL programming.\n· Knowledge on writing Packages, Stored procedures, Functions, Cursors. \n· Written SQL queries for DML operation. \n\n· Creating Tables, Views, Joins, Synonyms.\n\n· Creating and managing database objects, users, roles, privileges.\nProject Details:\nProject 5:\n\nClint                           :           NCR Corporation.\n\nProject Name            :           Line of Sight-2\n\nDatabase                    :           MS SQL source and Teradata as target\n\nDuration                    :           Sept 2018 to end\nRole                            :           Informatica & SAP BO\n\nDescription:\n\nThe purpose of this project is to support the initiative to develop Central repository to post Sales and Operations Planning and Line of sight source data with flexibility to load from different locations, at different times.\n\nThe Power BI and SAP BO Reports will give record of the billings to date, current backlog and forecast required for a rolling 12mths period.\n\nResponsibilities:\n\n· Created mappings and sessions to implement technical enhancements for data warehouse by extracting data from source like MS SQL and target as Teradata.\n· Created different Teradata tables and views.\n\n· Created different Hadoop tables as for the requirement. \n\n· Used various transformations like Source Qualifier, Expression, Aggregator, Joiner, Filter, Lookup, Update Strategy Designing and optimizing the Mapping.\n· Implemented new method to auto trigger the information flow when the data is available in the source.\n· Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor.\n· Eliminated different junk characters by writing custom query in the source qualifier transformation.   \n· Modified several of the existing mappings based on the user requirements and maintained existing mappings, sessions and workflows.\n· Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows.\n\n· Prepared the error handling document to maintain the error handling process.\n· Create SAP BO universe and reports as for the Business requirement.\n· Created objects and measures.\n\n· Used filters (Query filter and Report filter) and Prompts to restrict data in report.\n· Created predefine filters and report filters.\n\n· Worked on Webi functionalities like Break, Section, Sort, Drill, Charts, Filters, Variables, Merge of dimensions and Alerts.\n· Used different joins and looping between different dimension tables.\n· Created shell script for the auto trigged of the flow. \n· Prepared SQL Queries to validate the data in target databases to compare the data of the reports and tables data.\nProject4:\n\nClint                       :           NCR Corporation.\n\nProject Name        :           Quasar\n\nDatabase                :           Hadoop as Source and Teradata as target\n\nDuration\n                   :            Apr 2018 to Aug 2018\nRole                           :\n           Informatica developer\nDescription:\nQuasar is the universe which will have all the sales, order, product, shipment, Transition data and all other details.\nResponsibilities:\n\n· Excellent Experience with different indexes (PI, SI, JI, AJIs, PPI (MLPPI, SLPPI)) and Collect Statistics.\n\n· Implemented Agile scrum process in this project.\n\n· Created mappings and sessions to implement technical enhancements for data warehouse by using Hadoop as source and Teradata as target.\n\n·  Excellent experience in ETL Tools like Informatica and on implementing Slowly Changing Dimensions (SCD) and Incremental load.\n\n· As we are loading data from Hadoop using HDFS connection we are unable to use regular Incidental load. For this we have created new method to full fill that.    \n\n· Used various transformations like Source Qualifier, Expression, Joiner, Filter, Lookup, SQL, Update Strategy Designing and optimizing the Mapping.\n\n· Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor.\n\n· Modified several of the existing mappings based on the user requirements and maintained existing mappings, sessions and workflows.\n\n· Created different complex Query’s to get latest data.   \n\n· Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows.\n\n· Prepared SQL Queries to validate the data on both source and target databases.\n\nProject 3:\n\nClint                               :           NCR Corporation.\n\nProject Name                :           Product MDM\n\nDatabase                        :           XML as source, Teradata as target\n\nDuration\n                            :            Dec 2018-April 2018\nRole                                    :\n          Informatica & Unix developer\n\nDescription: \n\nThe Informatica Product 360 system/application will serve as the trusted repository of product master data within the organization as well as govern the standard that dependent systems shall comply with to ensure consistent collection of related transactional data. Ultimately the system’s goals are to 1) improve data quality 2) reduce complexity within related applications, and 3) increase agility and coordination  throughout the organization.\nResponsibilities:\n\n· Excellent Experience with different indexes (PI, SI, JI, AJIs, PPI (MLPPI, SLPPI)) and Collect Statistics.\n\n· Implemented Agile scrum process in this project.\n\n· Created mappings and sessions to implement technical enhancements for data warehouse by using XML as source and Teradata as target.\n\n· Excellent experience in ETL Tools like Informatica and on implementing Slowly Changing Dimensions (SCD).\n\n· Used various transformations like XML Source Qualifier, Expression, Aggregator, Joiner, Filter, Lookup, SQL, XML Parser, Update Strategy Designing and optimizing the Mapping.\n\n· Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor.\n\n· Modified several of the existing mappings based on the user requirements and maintained existing mappings, sessions and workflows.\n\n· Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows.\n\n· Prepared SQL Queries to validate the data on target databases.\n\n· Created Unix shell script to create different list of files based on the name of the file and sent an email to users after completing the load.\n\nProject 2:\n\nClint                :       NCR Corporation.\n\nProject Name :       Line of Sight-1\nDatabase         :      CSV fie source and Hadoop as target\nDuration\n           :\n   Oct 2015 to Dec 2015\n\nRole\n\n           :\n   Informatica & Tableau Developer\nDescription:\nThe purpose of this project is to support the initiative to develop Central repository to post Sales and Operations Planning and Line of sight source data with flexibility to load from different locations, at different times.\nThe tableau Reports will give record of the billings to date, current backlog and forecast required for a rolling 12mths period.\n\nResponsibilities:\n· Created mappings and sessions to implement technical enhancements for data warehouse by extracting data from source like Delimited Flat files and target as Hadoop.\n\n· Created different Hadoop tables as for the requirement. \n\n· Used various transformations like Source Qualifier, Expression, Aggregator, Joiner, Filter, Lookup, Update Strategy Designing and optimizing the Mapping.\n\n· Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor.\n\n· Modified several of the existing mappings based on the user requirements and maintained existing mappings, sessions and workflows.\n\n· Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows.\n\n· Prepared the error handling document to maintain the error handling process.\n\n· Created Unix shell script to move the CSV file from source directory (files that we received from FTS) to local directory’s and after the completion the load data into tables, moved those files into different directories based on the naming of the file.\n· Created different Tableau dashboard and story’s as for the business requirement.\n· Used Text table and Line and Bar charts for creating the reports.\n· Used all the aggregations functions like sum, running sum, max & min, average.\n\n· Used align or grouping functionless.\n\n· Created different calculation filed or Measures and parameters as for the requirement.  \n\n· Created common filter for the all the reports in the dashboard.\n\n· Used all the Marks functionalities.\n\n· Good in formatting the reports.\n· Prepared SQL Queries to validate the data in target databases to compare the data of the reports and tables data.\n· Created extract for the data set and published on the Tableau server.\n\n· Perform testing and fix bugs.\nProject 1:\n\nClint                       :           NCR Corporation.\n\nProject Name        :           ZBB BI Reporting.\nDatabase                :           Teradata\n\nDuration                :           Fed 2017 – Jun 2017\n\nRole                        :           SAP BO developer\nDescription:\nA method of budgeting in which all expenditures must be justified each new period, as opposed to only explaining the amounts requested in excess of the previous period’s funding…funding would have a base at zero. A department would have to show why its funding efficiently helps the organization toward its goals.”\nResponsibilities:\n· Created Classes and Objects at universe level.\n\n· Updated universe as for the specifications.\n\n· Created hierarchies to support users with the option to drill down on reports.\n\n· Worked on LOV’s in universe.\n· Changing the connections and publishing the universe to the repository.\n\n· Created Reports for data validation.\n\n· Used filters (Query filter and Report filter) and Prompts to restrict data in report.\n· Created predefine filters and report filters.\n\n· Worked on Webi functionalities like Break, Section, Sort, Drill, Charts, Filters, Variables, Merge of dimensions and Alerts.\n· Troubleshooting the issues in the report from the backend.\n· Involved in unit testing.\n\n· Created Design document and user document for Business purpose.\n· Worked on BO deployment process.\n· Involved in ZBB Hotline monitoring for solving the user issues on different reports in project.\n\n· Provided security access for the Business users.\nEducation:\n Bachelor’s in Engineering (ECE) from JNTU Kakinada.","annotation":[{"label":["Highest degree"],"points":[{"start":18670,"end":18719,"text":"Bachelor’s in Engineering (ECE) from JNTU Kakinada"}]},{"label":["Skills"],"points":[{"start":16892,"end":16894,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":15867,"end":16036,"text":"Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows"}]},{"label":["Total experience"],"points":[{"start":15577,"end":15727,"text":"Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor"}]},{"label":["Total experience"],"points":[{"start":15420,"end":15571,"text":"Used various transformations like Source Qualifier, Expression, Aggregator, Joiner, Filter, Lookup, Update Strategy Designing and optimizing the Mapping"}]},{"label":["Skills"],"points":[{"start":15378,"end":15383,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":15349,"end":15354,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":14686,"end":14691,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":14345,"end":14347,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":14161,"end":14330,"text":"Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows"}]},{"label":["Total experience"],"points":[{"start":13871,"end":14021,"text":"Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor"}]},{"label":["Skills"],"points":[{"start":13797,"end":13799,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":13450,"end":13579,"text":"Created mappings and sessions to implement technical enhancements for data warehouse by using XML as source and Teradata as target"}]},{"label":["Skills"],"points":[{"start":12371,"end":12373,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":12187,"end":12356,"text":"Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows"}]},{"label":["Total experience"],"points":[{"start":11837,"end":11987,"text":"Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor"}]},{"label":["Skills"],"points":[{"start":11775,"end":11777,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":11550,"end":11555,"text":"Hadoop"}]},{"label":["Total experience"],"points":[{"start":11522,"end":11677,"text":"As we are loading data from Hadoop using HDFS connection we are unable to use regular Incidental load. For this we have created new method to full fill that"}]},{"label":["Skills"],"points":[{"start":11348,"end":11353,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":10770,"end":10775,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":10520,"end":10522,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":9761,"end":9930,"text":"Tuned the performance of mappings by following Informatica best practices and also applied several methods to get best performance by decreasing the run time of workflows"}]},{"label":["Total experience"],"points":[{"start":9367,"end":9517,"text":"Developed Workflows using task developer, work let designer, and workflow designer in Workflow manager and monitored the results using workflow monitor"}]},{"label":["Total experience"],"points":[{"start":9107,"end":9258,"text":"Used various transformations like Source Qualifier, Expression, Aggregator, Joiner, Filter, Lookup, Update Strategy Designing and optimizing the Mapping"}]},{"label":["Skills"],"points":[{"start":9065,"end":9070,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":8969,"end":8971,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":8283,"end":8285,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":7951,"end":7953,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":7851,"end":7853,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":7844,"end":7846,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":7837,"end":7839,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":7809,"end":7811,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":7806,"end":7818,"text":"MS SQL Server"}]},{"label":["Skills"],"points":[{"start":6447,"end":6449,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":5393,"end":5395,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":4947,"end":5262,"text":"Experience in building dashboards using objects like Text tables, Box plots, whisker plots, heat maps, spider charts, tree maps, bullet charts, control charts, trellis charts, waterfall charts, Line, Dual Line chat, Ternary charts, Radial charts, Kaplan-Meier curve, bubble charts, symbol map, area charts in Tableau"}]},{"label":["Skills"],"points":[{"start":4490,"end":4495,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":4475,"end":4477,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":3996,"end":4105,"text":"Excellent Experience with different indexes (UPI, PI, SI, JI, AJIs, PPI (MLPPI, SLPPI)) and Collect Statistics"}]},{"label":["Skills"],"points":[{"start":3982,"end":3984,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3915,"end":3917,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":3864,"end":3990,"text":"Proficient in performance analysis, monitoring and SQL query tuning using EXPLAIN PLAN, Collect Statistics, Hints and SQL Trace"}]},{"label":["Skills"],"points":[{"start":3856,"end":3858,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":2980,"end":3168,"text":"Creating and scheduling Sessions and Batch Process based on demand, run on time, or run only once using Informatica Workflow Manager and monitoring the data loads using the Workflow Monitor"}]},{"label":["Skills"],"points":[{"start":2775,"end":2777,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":2507,"end":2777,"text":"Developed Mappings using Transformations like Source Qualifier, Expression, Aggregator, Connected & unconnected lookups, Filter, Router, Rank, Sequence Generator, Sorter, Joiner, Stored Procedure, Normalizer and Router transformations and Update Strategy, XML Parser, SQL"}]},{"label":["Skills"],"points":[{"start":2341,"end":2343,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2330,"end":2335,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":2263,"end":2265,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":2206,"end":2413,"text":"Extract and transform data from multiple sources such as SQL Server, Teradata, flat file, XML etc., and load into Teradata, Hadoop, MS SQL server tables which were used for reporting and sales force analytics"}]},{"label":["Total experience"],"points":[{"start":2029,"end":2160,"text":"Knowledge in developing various Mappings with the collection of all Sources, Targets, and transformations using INFORMATICA Designer"}]},{"label":["Skills"],"points":[{"start":1979,"end":1990,"text":"SAP BOBJ 4.X"}]},{"label":["Skills"],"points":[{"start":1970,"end":1976,"text":"TABLEAU"}]},{"label":["Skills"],"points":[{"start":1904,"end":1920,"text":"SAP DATA SERVICES"}]},{"label":["Skills"],"points":[{"start":1794,"end":1799,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":1773,"end":1790,"text":"ORACLE  11g(Basics"}]},{"label":["Skills"],"points":[{"start":1761,"end":1763,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1758,"end":1770,"text":"MS SQL SERVER"}]},{"label":["Skills"],"points":[{"start":1714,"end":1717,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":1709,"end":1711,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":1705,"end":1707,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":1403,"end":1589,"text":"Excellent problem-solving skills with strong technical background and good interpersonal skills. Quick learner and excellent team player, ability to meet deadlines and work under pressure"}]},{"label":["Total experience"],"points":[{"start":1142,"end":1182,"text":"Expertise in creating SAP BO WEBI reports"}]},{"label":["Total experience"],"points":[{"start":879,"end":1030,"text":"Hands on experience in tuning mappings, identifying and resolving performance bottlenecks in various levels like sources, targets, mappings and sessions"}]},{"label":["Total experience"],"points":[{"start":792,"end":873,"text":"Proficient in Dimensional Data Modelling Concepts-Star and Snowflake schema design"}]},{"label":["Skills"],"points":[{"start":773,"end":775,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":762,"end":767,"text":"Hadoop"}]},{"label":["Total experience"],"points":[{"start":655,"end":787,"text":"Extensively worked on data extraction, transformation and loading data from various sources like Teradata, Hadoop, MS SQL, Flat files"}]},{"label":["Total experience"],"points":[{"start":574,"end":649,"text":"Expertise in ETL Development Process using Informatica and SAP Data Services"}]},{"label":["Total experience"],"points":[{"start":506,"end":568,"text":"Having 2 Years of working experience in Informatica Development"}]},{"label":["Total experience"],"points":[{"start":324,"end":484,"text":"To use my skills and talent to learn new things and be a key part in the development of the organization besides gaining practical exposure of various components"}]},{"label":["Phone"],"points":[{"start":167,"end":176,"text":"7780188827"}]},{"label":["Email"],"points":[{"start":51,"end":75,"text":"alladaveerendra@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":18,"text":"VEERENDRA DURGA RAM"}]}],"extras":null,"metadata":{"first_done_at":1624272247000,"last_updated_at":1624272247000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Name: Santosh Jaiswal\nEmail  Id :  santoshativ@gmail.com\nMobile No : 7903601794\nExperience Summary:\n· Experienced and expert IT professional possess expertise in handling diverse \n\n             IT Initiatives.\n\n· Expertise in incident management and problem management.\n\n· Experienced in supporting multiple applications with many production and   \n\n             staging Database servers.\n\n· The record of accomplishment of achieving company objectives effectively.\n\n· Handling of different tickets and resolving within SLA.\nSkills:\n\tTechnical Skills\n\tSQL Server Database Administrator,SQL Developer,PLSQL Developer Informatica Developer\n\n\tTools Used\n\tSSMS, Perfomance Monitor, Tuning Advisor, SQL Configution Manager,SQL Nexus,  Power BI Desktop, Power Bi Report Server, Failover Cluster Manager,SQL Developer, Putty, Informatica Power Centre \n\n\tMethods\n\tWaterfall model, Agile Methodology\n\n\tDomain\n\tHitech, Healthcare\n\n\nWork Experience:\nWork Experience of 2.5 years as Support Executive for a DBA Role, SQL Developer &  4 months as a PLSQL and Informatica developer\n\nKey Responsibilities Handled:\n· Performance, integrity and security of a database. \n\n· Planning and development of the database, as well as in \n\n           troubleshooting  any issues on behalf of any users.\n· Ticket handling and resolving within the SLA\n· Ensure that the DB server health is up to the level and the \n\n           responding well.\n\n· DB server analysis in case the server is responding slow : checking \n\n           the blockings \\Deadlocks, Perfmon Counters, Event viewers etc.\n\n· Setting up high availability solutions for the SQL server like \n\n· Database  mirroring, Replication, Failover clustering, Log shipping            \n\n           & Disaster Recovery\n\n· DB migration activities from on premise server to Azure VM servers.\n\n· Data Backup & Recovery, DB mail Configuration\n\n· Installation and Configuration of SQL server\n\n· Creation of complex Stored Procs(SQL+Oracle) for the Data Movement\n\n· Creation of Mappings and Workflows in Informatica Power Centre for \n\n           ETL Process\nKey Projects Undertaken:\nProject: KPMG International \nRole: SQL server DBA (Support 24*7)\nSoftware Engineering Methodologies: Waterfall model\nDesignation:Support Executive\nDescription:  Responsible for the performance, integrity and security of a database. \n\nTicket handling and resolving the same within SLA. As a DBA have supported more than 100 database servers,capacity Planning, as well as in troubleshooting of any issues on behalf of the users. To ensure data remains consistent across the database. Setting up high availability solutions for the SQL server like Database mirroring, Replication, Failover clustering, Log shipping & Disaster Recovery etc\nProject: Humana\nRole:  Informatica & PLSQL Developer\nSoftware Engineering Methodologies: Agile\nDesignation: Developer\nDescription: Involved in creation of different mappings, workflows based on the requirements in Informatica Power centre and complex stored procs\nKey Projects Achievements & Appreciations\n\tAward Name\n\tAward Date\n\tAward Category\n\tAward SubCategory\n\n\tOn The Spot Award\n\t03/20/19\n\tAward for the excellent project work\n\tOn The Spot Awards\n\n\tAutomation in Perfmon Counters\n\t05/05/19\n\tScheduled Report geneartion for the SQL counters and threshold value alert mail \n\tOn The Spot Awards\n\n\nEducation Details:\n· Bachelor of Engineering in Electrical Engineering with 84%.\n\n· HSC from Board of secondary education with 72%.\n\n· SSC from Board of secondary education with 81%.\nPersonal Qualities:\n· Strong willingness to learn new things.\n\n· Team player with a work ethics, Committed to work hard, smarter and sincerely.\n\n· Fast adaptability to new challenge and responsibilities.\n\n· Strong visualization, problem solving and analytical skills\n\n· Ability to work on Rotational Shifts\n\n· Team-focused and patient.\n\n· Possess creative artistic mind.\n\n· Ability to work as team and individually.\nDeclaration:\nI hereby declare that to the best of my knowledge and belief the information given above is true and correct. If given me an opportunity to work in organization, I will put my best efforts.","annotation":[{"label":["10 %"],"points":[{"start":3474,"end":3519,"text":"SSC from Board of secondary education with 81%"}]},{"label":["12 %"],"points":[{"start":3423,"end":3468,"text":"HSC from Board of secondary education with 72%"}]},{"label":["Highest degree"],"points":[{"start":3360,"end":3417,"text":"Bachelor of Engineering in Electrical Engineering with 84%"}]},{"label":["Skills"],"points":[{"start":3272,"end":3274,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2778,"end":2780,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2632,"end":2634,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2138,"end":2140,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1949,"end":1951,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1902,"end":1904,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":1748,"end":1813,"text":"DB migration activities from on premise server to Azure VM servers"}]},{"label":["Skills"],"points":[{"start":1613,"end":1615,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":1419,"end":1561,"text":"DB server analysis in case the server is responding slow : checking \n\n           the blockings \\Deadlocks, Perfmon Counters, Event viewers etc."}]},{"label":["Total experience"],"points":[{"start":1326,"end":1413,"text":"Ensure that the DB server health is up to the level and the \n\n           responding well"}]},{"label":["Skills"],"points":[{"start":1038,"end":1040,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1005,"end":1007,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":939,"end":1066,"text":"Work Experience of 2.5 years as Support Executive for a DBA Role, SQL Developer &  4 months as a PLSQL and Informatica developer"}]},{"label":["Skills"],"points":[{"start":797,"end":799,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":730,"end":745,"text":"Power BI Desktop"}]},{"label":["Skills"],"points":[{"start":718,"end":720,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":694,"end":696,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":652,"end":655,"text":"SSMS"}]},{"label":["Skills"],"points":[{"start":602,"end":604,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":586,"end":588,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":552,"end":570,"text":"SQL Server Database"}]},{"label":["Skills"],"points":[{"start":552,"end":554,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":392,"end":463,"text":"The record of accomplishment of achieving company objectives effectively"}]},{"label":["Total experience"],"points":[{"start":213,"end":267,"text":"Expertise in incident management and problem management"}]},{"label":["Phone"],"points":[{"start":69,"end":78,"text":"7903601794"}]},{"label":["Email"],"points":[{"start":35,"end":55,"text":"santoshativ@gmail.com"}]},{"label":["Name"],"points":[{"start":6,"end":20,"text":"Santosh Jaiswal"}]}],"extras":null,"metadata":{"first_done_at":1624273701000,"last_updated_at":1624273701000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Vishakha Sheoran                                                                      \nMobile: 91-7387983184\nEmail: vishuakka01@gmail.com\nProfessional Profile\n· 4.5+ years of IT experience in Analysis, Design, Development, Implementation, Data Integration Solutions and BI tools.\n· Experience in data analysis, data migration, data integration, dimensional modeling, database design and development.\n· Extensive design, development and testing experience with Informatica Power Center 9.1/9.6 and 10.\n· Worked on Talend open studio to generate the reports from complex data base and files.\n· Proficient in designing and developing the various transformation logics.\n· Involved in Unit Testing, Peer review of the ETL code and in building the Unit Test case documents.\n· Proficient in developing SQL with relational database like Oracle.\n· Strong relational database knowledge Oracle PL/SQL, SQL Server 2008\n· Configure, Schedule and Monitor jobs using   shell scripts in UNIX. \n· Independently identify bottlenecks, perform complex troubleshooting, performance tuning, root-cause analysis and solution development. Involved in performance tuning of the ETL code.\n· Work experience of the projects of Telecom Industry in both Agile & Waterfall Methodology.\n· Worked on data quality and data profiling.\n· Skilled in implementing efforts towards enhancements and maintenance of previous implementation and resolving on-going issues.\n· Ability to meet deadlines and handle multiple tasks, decisive with strong leadership qualities, flexible in work schedules and possess good communication and interpersonal skills. \n· Good knowledge on Data warehousing and Business intelligence concepts with strong knowledge on Data modeling design e.g. Star, Snowflake design \n· Resourceful, creative problem-solver with proven aptitude to analyze and translate complex customer requirements and business problems and design/implement innovative customer solutions.\n· Motivated achiever who exceeds goals, has been promoted regularly, earned numerous customer appreciations and has garnered numerous performance awards.\n\nTechnical Profile\n\n   Programming Languages\n· SQL, PL/SQL, UNIX\n   Database\n· Oracle 11g\n   Development tools\n· Informatica Power Center, Toad, Putty, WinSCP\n  Operating systems\n· UNIX, AIX, Microsoft Win XP, Win 7, Win 10\n\n\nProfessional Experience\n\nProject#2                   Client: Centurylink, US\n\t\t\t  Software: Informatica PowerCenter\n\t\t\t  Role: Informatica Developer\n\t\t\t  Domain: Telecom.\n\t\t\t  Duration: Nov-2017 till date\n\n\n\nProject Description:\n\nCenturyLink, Inc. is an American telecommunications company, headquartered in Monroe, Louisiana, that provides communications and data services to residential, business, governmental, and wholesale customers in 37 states. A member of the S&P 500 index, the company operates as a local exchange carrier and Internet access provider in U.S. markets and is the third-largest telecommunications company in the United States in terms of lines served, behind AT&T and Verizon. It also provides long distance service.\nRoles & Responsibilities:\n\n· Design, Develop and Test ETL Mappings, Mapplets, Workflows, Worklets using Informatica PowerCenter 9.x.\n· Work in a fast-paced environment, under minimal supervision providing technical guidance to the team members.\n· Responsible for database schema design, integration testing and other projects that may be necessary to help the team achieve their goals.\n· Working closely with Onshore and offshore application development leads.\n· ETL Mappings, Mapplets, Workflows, Worklets using Informatica PowerCenter 9.x. \n· Identify efficiencies and ways to improve design and development processes.\n· Identify ways to increase efficiency of production support - Find solutions that allow operations to better do their job without involving development resource time.\n· Plans work and leads team of others; mentors others on the team with application functionality.\n\nProject#1                  Client: IDEA\n\t          Software: Oracle 11g\n\t\t          Role: Pl/SQL Developer\n\t\t          Domain: Telecom\n\t\t          Duration: Feb-2015 to Nov 2017\nProject Description:\n\nThe Aditya Birla Group is an Indian multinational conglomerate, headquartered in World, Mumbai, India. It operates in 35 countries with more than 120,000 employees worldwide. The group was founded by Seth Shiv Narayan Birla in 1857. The group has interests in sectors such as viscose staple fiber, metals, cement (largest in India), viscose filament yarn, branded apparel, carbon black, chemicals, fertilizers, insulators, financial services, telecom BPO and IT services.\n\nRoles & Responsibilities:\n\n· As Data Migration Team member my role is to perform the development of recurring CRs (i.e. Change requests) using the SQL queries and UNIX shell scripting.\n· Leading other team members.\n· Root cause analysis.\n· Ticket Handling and Bug Fixing.\n· Helping other team members in developing PL/SQL script or Unix shell script as and when required.\n· Validate the data before and after Migrations.\n· Tuning Queries.\n· Creating index on tables to improve query performance.\n· Running SQL loader to load data into Database using control files.\n· Send reports to Stake Holders and Attend to any queries posed by Client.\n\nAcademic Credentials\n\nB.E from GTU UNIVERSITY (Electronics & Communication)","annotation":[{"label":["Highest degree"],"points":[{"start":5294,"end":5346,"text":"B.E from GTU UNIVERSITY (Electronics & Communication)"}]},{"label":["Skills"],"points":[{"start":5137,"end":5139,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4949,"end":4951,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4946,"end":4951,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":4778,"end":4780,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":4660,"end":4797,"text":"As Data Migration Team member my role is to perform the development of recurring CRs (i.e. Change requests) using the SQL queries and UNIX"}]},{"label":["Skills"],"points":[{"start":4051,"end":4053,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4019,"end":4028,"text":"Oracle 11g"}]},{"label":["Total experience"],"points":[{"start":3099,"end":3201,"text":"Design, Develop and Test ETL Mappings, Mapplets, Workflows, Worklets using Informatica PowerCenter 9.x."}]},{"label":["Total experience"],"points":[{"start":2559,"end":3067,"text":"CenturyLink, Inc. is an American telecommunications company, headquartered in Monroe, Louisiana, that provides communications and data services to residential, business, governmental, and wholesale customers in 37 states. A member of the S&P 500 index, the company operates as a local exchange carrier and Internet access provider in U.S. markets and is the third-largest telecommunications company in the United States in terms of lines served, behind AT&T and Verizon. It also provides long distance service"}]},{"label":["Skills"],"points":[{"start":2182,"end":2191,"text":"Oracle 11g"}]},{"label":["Skills"],"points":[{"start":2158,"end":2160,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2155,"end":2160,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":2150,"end":2152,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":1762,"end":1946,"text":"Resourceful, creative problem-solver with proven aptitude to analyze and translate complex customer requirements and business problems and design/implement innovative customer solutions"}]},{"label":["Total experience"],"points":[{"start":1432,"end":1609,"text":"Ability to meet deadlines and handle multiple tasks, decisive with strong leadership qualities, flexible in work schedules and possess good communication and interpersonal skills"}]},{"label":["Skills"],"points":[{"start":891,"end":893,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":886,"end":888,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":883,"end":888,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":795,"end":797,"text":"SQL"}]},{"label":["Total experience"],"points":[{"start":402,"end":498,"text":"Extensive design, development and testing experience with Informatica Power Center 9.1/9.6 and 10"}]},{"label":["Total experience"],"points":[{"start":282,"end":397,"text":"Experience in data analysis, data migration, data integration, dimensional modeling, database design and development"}]},{"label":["Total experience"],"points":[{"start":161,"end":277,"text":"4.5+ years of IT experience in Analysis, Design, Development, Implementation, Data Integration Solutions and BI tools"}]},{"label":["Email"],"points":[{"start":116,"end":136,"text":"vishuakka01@gmail.com"}]},{"label":["Phone"],"points":[{"start":98,"end":107,"text":"7387983184"}]},{"label":["Name"],"points":[{"start":0,"end":15,"text":"Vishakha Sheoran"}]}],"extras":null,"metadata":{"first_done_at":1624272653000,"last_updated_at":1624272653000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Mark Paul\nETL Developer\n\nContact: +91 8840649661\nEmail: markpaul458@gmail.com\n\n\t\n\t\n\n\t\n\t\n\t\n\t\n\n\t\n\t\n\t\n\t\n\t\n\n\t\n\tTotal Experience\n2 Years 11 Months  in BI/DW\n--------------------------------------\nQualification\nB.E. Computer Engineering\n-------------------------------------\nWork history\nINFOCEPTS\n(Jan 2017 - Present) \n-------------------------------------\nWork Summary\n· End to End ETL Design and Development including Testing\n\n· Data Warehousing Design\n\n\n\n-------------------------------------\n\nReward and Recognition\n\n· InfoCepts Best Interviewee RunnerUp\n· InfoCepts Gem of the Month July 2018\n· BootCamp Top Mentor Award (for three BootCamp batches)\n· Rewarded by UP Sate Government for scoring 100 in Matematics in 12th Standard Exam\n\n-------------------------------------\n\n\t\n\tSummary and Accomplishments\n· 2+ Years of delivering analytical and problem solving skills and ability to follow through with projects from inception to completion\n· Proven ability to successfully work for multiple requirements, design, and development approaches and methodologies (Agile, Waterfall, etc.)\n· Strong experience in Full Life-cycle BI and Data warehousing projects inclusive of requirements analysis, proof-of-concepts, design, development, testing and implementation \n· Extensive experience in creating Informatica Mappings,Sessions and Worlflows. Creating Database objects like Stored Procedures and Functions, and scheduling the ETL jobs.\n· Building Batch or Powershell scripts for requirements which cannot be fulfilled directly using Informatica\n· Have a strong command over SQL\n· Good amount of experience in creating ST Mappings and Test Case documentation\n· Performing ETL Testing \n· Testing the BI Reports by comparing the SQL generated by Reporting tool with manually written SQL\n· Performance tuning of Informatica Sessions using Pushdown Optimisation/Partitioning\n· SCD2/SCD1 Implementation experience\n\t\n\n\t\n\t\n\t\n\tTechnical Skills\n\t\n\n\t\n\n\t\n\t\n\t\n\tBusiness Intelligence Tools\n\tMicroStrategy ,Tableau\n\t\n\n\t\n\t\n\t\n\tData Modeling \n\tNormalized, De-normalized, Star and Snow Flake Schemas\n\t\n\n\t\n\t\n\t\n\tETL Tool\n\tInformatica Powercenter\n\t\n\n\t\n\t\n\t\n\tDatabases\n\tMS-SQL Server, Netezza,My SQL\n\t\n\n\t\n\t\n\t\n\tInternet Technologies\n\tHTML,CSS,Java Script,PHP,Java Servlet,JSP\n\t\n\n\t\n\t\n\t\n\tProgramming Languages\n\t Python,Java,C,C++\n\t\n\n\t\n\t\n\t\n\tOperating Systems\n\tWindows 32bit / 64 bit,Linux\n\n\t\n\n\t\n\t\n\t\n\tShell Scripting\n\tBatch,Powershell,Unix\n\t\n\n\t\n\t\n\nProject Profiles\n\t\n\n\t\n\tProject Details\nInfoCepts\n< InfoCepts MIS>\n< March 17 to March 19>\n\nRole\nInformatica/SQL Developer\n\t\n\tResponsibilities\n· Building Data Warehouse for InfoCepts \n· Integrating all the In-house applications and bring all the Internal data into a centralized data warehouse\n· Creating required Informatica and Database objects, with documentation\n· Testing the code in DEV and QA environments, and migrating to Production\n· Daily Monitoring the ETL jobs and take required actions in case of failures\n· Interacting with the Clients to understand the requirements, while working in Agile mode\n· Assist the Reporting team in understanding the schema changes and building reports\n· Performance tuning of ETL Jobs\n\t\n\n\t\n\tProject Details \nPetSmart Inc. ( https://www.petsmart.com/)\n< Reflexis Upgrade>\nApril 19 – Present\n\nRole\nInformatica/SQL Developer\n\t\n\tResponsibilities\n· Creating mappings for loading data in new tables\n· Creating Views to do a union of old and new tables\n· Creating workflow,sessions, worklets and powershell scripts\n· Testing the code and creating Test case documents\n· Creating Deployment Groups for migration purpose\n· Raising Issues and Challenges to Client in Stand Up Calls and giving daily work updates\n· Interacting with the Clients to understand the requirements, while working in Agile mode\n· Assist the Reporting team in understanding the schema changes and building reports\n· Daily Monitoring the ETL jobs and helping support teams in recovering the failures\n\n\t\n\n\n\nInternship/College Projects\n· MicroStrategy based “World Terrorism Analysis Dashboard”: \nA MicroStrategy based Reporting Solution, which gave information about last fifty years terrorism activities around the Globe. This dashboard provided Geography wise active Terrorist group past activities, nature of attacks, number of casualities and weapons used. The user could drill down from Region to Country to State to City level analysis.\n· Tableau Based “US Traffic Analysis Dashboard”:\nA Tableau Reporting solution intended to help Traffic Authorities do analysis over the US Traffic and Accidents over various US States.\n· File Sharing Andriod app: \nA mini project intended to build a capability to share files over Wifi and Internet, using an android application.\n· Web Development Projects: \nCollege Technical Events website development project. Working upon server side scripts based upon PHP.\n· Java JFRAME based stand alone java utilities.\n  \t         Page 1","annotation":[{"label":["Skills"],"points":[{"start":4827,"end":4830,"text":"Java"}]},{"label":["Skills"],"points":[{"start":4820,"end":4822,"text":"PHP"}]},{"label":["Skills"],"points":[{"start":4722,"end":4722,"text":"C"}]},{"label":["Skills"],"points":[{"start":4415,"end":4421,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":4366,"end":4372,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":4343,"end":4343,"text":"C"}]},{"label":["Skills"],"points":[{"start":4323,"end":4323,"text":"C"}]},{"label":["Skills"],"points":[{"start":4019,"end":4031,"text":"MicroStrategy"}]},{"label":["Skills"],"points":[{"start":3958,"end":3970,"text":"MicroStrategy"}]},{"label":["Skills"],"points":[{"start":3939,"end":3939,"text":"C"}]},{"label":["Skills"],"points":[{"start":3684,"end":3684,"text":"C"}]},{"label":["Skills"],"points":[{"start":3625,"end":3625,"text":"C"}]},{"label":["Skills"],"points":[{"start":3606,"end":3606,"text":"C"}]},{"label":["Skills"],"points":[{"start":3592,"end":3592,"text":"C"}]},{"label":["Skills"],"points":[{"start":3522,"end":3522,"text":"C"}]},{"label":["Skills"],"points":[{"start":3408,"end":3408,"text":"C"}]},{"label":["Skills"],"points":[{"start":3355,"end":3355,"text":"C"}]},{"label":["Skills"],"points":[{"start":3304,"end":3304,"text":"C"}]},{"label":["Skills"],"points":[{"start":2959,"end":2959,"text":"C"}]},{"label":["Skills"],"points":[{"start":2712,"end":2712,"text":"C"}]},{"label":["Total experience"],"points":[{"start":2602,"end":2708,"text":"Integrating all the In-house applications and bring all the Internal data into a centralized data warehouse"}]},{"label":["Skills"],"points":[{"start":2593,"end":2593,"text":"C"}]},{"label":["Skills"],"points":[{"start":2472,"end":2472,"text":"C"}]},{"label":["Skills"],"points":[{"start":2460,"end":2460,"text":"C"}]},{"label":["Skills"],"points":[{"start":2297,"end":2299,"text":"C++"}]},{"label":["Skills"],"points":[{"start":2297,"end":2297,"text":"C"}]},{"label":["Skills"],"points":[{"start":2295,"end":2295,"text":"C"}]},{"label":["Skills"],"points":[{"start":2290,"end":2293,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2283,"end":2288,"text":"Python"}]},{"label":["Total experience"],"points":[{"start":2232,"end":2243,"text":"Java Servlet"}]},{"label":["Skills"],"points":[{"start":2232,"end":2235,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2228,"end":2230,"text":"PHP"}]},{"label":["Skills"],"points":[{"start":2216,"end":2226,"text":"Java Script"}]},{"label":["Skills"],"points":[{"start":2216,"end":2219,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2212,"end":2214,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":2212,"end":2212,"text":"C"}]},{"label":["Skills"],"points":[{"start":2207,"end":2210,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":2167,"end":2172,"text":"My SQL"}]},{"label":["Skills"],"points":[{"start":2159,"end":2165,"text":"Netezza"}]},{"label":["Skills"],"points":[{"start":2144,"end":2156,"text":"MS-SQL Server"}]},{"label":["Skills"],"points":[{"start":1990,"end":1996,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":1975,"end":1987,"text":"MicroStrategy"}]},{"label":["Skills"],"points":[{"start":1876,"end":1876,"text":"C"}]},{"label":["Skills"],"points":[{"start":1871,"end":1871,"text":"C"}]},{"label":["Skills"],"points":[{"start":1637,"end":1637,"text":"C"}]},{"label":["Total experience"],"points":[{"start":1578,"end":1654,"text":"Good amount of experience in creating ST Mappings and Test Case documentation"}]},{"label":["Skills"],"points":[{"start":1341,"end":1341,"text":"C"}]},{"label":["Total experience"],"points":[{"start":1263,"end":1431,"text":"Extensive experience in creating Informatica Mappings,Sessions and Worlflows. Creating Database objects like Stored Procedures and Functions, and scheduling the ETL jobs"}]},{"label":["Total experience"],"points":[{"start":1087,"end":1258,"text":"Strong experience in Full Life-cycle BI and Data warehousing projects inclusive of requirements analysis, proof-of-concepts, design, development, testing and implementation"}]},{"label":["Total experience"],"points":[{"start":944,"end":1083,"text":"Proven ability to successfully work for multiple requirements, design, and development approaches and methodologies (Agile, Waterfall, etc.)"}]},{"label":["Total experience"],"points":[{"start":808,"end":940,"text":"2+ Years of delivering analytical and problem solving skills and ability to follow through with projects from inception to completion"}]},{"label":["Skills"],"points":[{"start":636,"end":636,"text":"C"}]},{"label":["Skills"],"points":[{"start":599,"end":599,"text":"C"}]},{"label":["Skills"],"points":[{"start":560,"end":560,"text":"C"}]},{"label":["Skills"],"points":[{"start":522,"end":522,"text":"C"}]},{"label":["Total experience"],"points":[{"start":367,"end":421,"text":"End to End ETL Design and Development including Testing"}]},{"label":["Skills"],"points":[{"start":286,"end":286,"text":"C"}]},{"label":["Experience in current company"],"points":[{"start":282,"end":290,"text":"INFOCEPTS"}]},{"label":["Skills"],"points":[{"start":210,"end":210,"text":"C"}]},{"label":["Highest degree"],"points":[{"start":205,"end":229,"text":"B.E. Computer Engineering"}]},{"label":["Email"],"points":[{"start":56,"end":76,"text":"markpaul458@gmail.com"}]},{"label":["Phone"],"points":[{"start":38,"end":47,"text":"8840649661"}]},{"label":["Skills"],"points":[{"start":25,"end":25,"text":"C"}]},{"label":["Name"],"points":[{"start":0,"end":8,"text":"Mark Paul"}]}],"extras":null,"metadata":{"first_done_at":1624275179000,"last_updated_at":1624275179000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "AYUSHI SAKLANI \n \n\n \n\n \n\n \n \n\nAbout me: \n\n Quick Learner \n\n Multi-Tasking \n\n Team Player \n\n Adaptable \n\n Passionate \n\n \n\nE-mail: \n\nsaklaniayushi@gmail. com \n\n \n\nContact No: \n\n+91-8882565457 \n\n \n \n\nPersonal Details:  \n\nFather’s Name: Ramesh \n\nSaklani \n\nDate of Birth: 01-Jan-1994 \n\n \n \n\nAddress: \n\nD-361 Street no 12 Near \n\nLaxminagar metro station, \n\nLaxminagar New Delhi \n\nCareer Objective \n\n \nTo work with an organization where I can utilize my skills in an \n\nefficient manner and help the company to grow further and to \n\nprove myself worthy of shouldering the responsibility assigned to \n\nme. \n\n \nProfile Summary \n\nSystem Engineer with around 3.5 years of experience in \nJava /J2EE. \n\n \n\nKey Responsibilities: \n\n \n\n Design, build, and maintain efficient, reusable, and \nreliable Java code. \n\n Contributing in all phases of the development lifecycle. \n\n Extensively worked with Java Collection classes \nlike ArrayList , HashMap etc. \n\n Release Management for first Release and \nenhancements. \n\n Involved in writing multi-table joins, \nprocedures, function and triggers. \n\n Support formal testing and resolve test defects. \n\n \nExpertise: \n\n \n Primary: Java, JDBC, Collections, Multithreading. \n\n Additional: pl/sql . \n\n Software tools: Eclipse, PL/SQL Developer. \n\n Platforms/Data Base: Oracle 10g \n\n Web Environments: Oracle Weblogic \n\n Concepts: Object Oriented Design. \n\n\n\nCorporate Experience: \n \n\n \nOrganization \n\n \nTata Consultancy Services Ltd. \n\n \nPeriod \n\n \nDecember, 2015 to present \n\n \nProject \n\n \nIndian Government Revenue System \n\n \nTechnical Skills \n\nJava, TCS MasterCraft Enterprise App Maker \n\n \nDescription \n\n \n  Next Generation automation application for Income Tax             \n  Department, CBDT, Govt. of India. \n\n \n\n \nProfile Overview \n\n \n Utilised technical expertise in various technologies(JAVA \n\n,PL/SQL,J2EE) for development of Income Tax Business \nApplication (ITBA) modules. \n\n Proficient  with  tools  like  Eclipse,  TCS  MasterCraft \nEnterprise Appmaker, PL/SQL developer. \n \n\n Aligned in production and enhancement releases \n\n Developed Java batch to process bulk data in production \n\n \n Displayed Problem Solving capability peered with strong \n\nCommunication skills. \n \nTechnical assistance to functional team while gathering the \nrequirements from clients to design and implement business \nprocesses. \n \n\n \n\nEXTRA-CURRICULAR ACTIVITIES/RECOGNITION \n\n \n\n Awarded as “Star of the month” in current project. \n\n Best Team award to our team. \n\n \n\n \n \n\n \n\n \n\n\n\nACADEMIC DETAILS  \n\n \n\n B.E. in Computer Science and Technology from G. B. Pant Engg. College Pauri, UTU in 2015. \n\n 12th  from SGRR, CBSE in 2011. \n\n 10th  from S.S.Niketan, State Board in 2009.","annotation":[{"label":["10 %"],"points":[{"start":2670,"end":2712,"text":"10th  from S.S.Niketan, State Board in 2009"}]},{"label":["12 %"],"points":[{"start":2635,"end":2663,"text":"12th  from SGRR, CBSE in 2011"}]},{"label":["Highest degree"],"points":[{"start":2541,"end":2628,"text":"B.E. in Computer Science and Technology from G. B. Pant Engg. College Pauri, UTU in 2015"}]},{"label":["Total experience"],"points":[{"start":2083,"end":2137,"text":"Developed Java batch to process bulk data in production"}]},{"label":["Skills"],"points":[{"start":1959,"end":1965,"text":"Eclipse"}]},{"label":["Experience in current company"],"points":[{"start":1440,"end":1468,"text":"Tata Consultancy Services Ltd"}]},{"label":["Skills"],"points":[{"start":1337,"end":1351,"text":"Oracle Weblogic"}]},{"label":["Skills"],"points":[{"start":1304,"end":1313,"text":"Oracle 10g"}]},{"label":["Skills"],"points":[{"start":1252,"end":1258,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":1223,"end":1228,"text":"pl/sql"}]},{"label":["Total experience"],"points":[{"start":1008,"end":1080,"text":"Involved in writing multi-table joins, \nprocedures, function and triggers"}]},{"label":["Total experience"],"points":[{"start":726,"end":797,"text":"Design, build, and maintain efficient, reusable, and \nreliable Java code"}]},{"label":["Total experience"],"points":[{"start":400,"end":599,"text":"To work with an organization where I can utilize my skills in an \n\nefficient manner and help the company to grow further and to \n\nprove myself worthy of shouldering the responsibility assigned to \n\nme"}]},{"label":["Address"],"points":[{"start":302,"end":375,"text":"D-361 Street no 12 Near \n\nLaxminagar metro station, \n\nLaxminagar New Delhi"}]},{"label":["Phone"],"points":[{"start":184,"end":193,"text":"8882565457"}]},{"label":["Email"],"points":[{"start":136,"end":159,"text":"saklaniayushi@gmail. com"}]},{"label":["Name"],"points":[{"start":0,"end":13,"text":"AYUSHI SAKLANI"}]}],"extras":null,"metadata":{"first_done_at":1624273418000,"last_updated_at":1624273418000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Aparna Bhatia\nSoftware Engineer \nTata Consultancy Services, Noida\nB-Tech Electronics and Communication Engineering, Hindustan College of Science and Technology| bhatiaaparna18@gmail.com |\nWORK EXPERIENCE\n· Total 4 years 9 months experience in Software Development.\n· Currently working as a Software Engineer in Tata Consultancy Services, Noida\n· Key expertise: Software development based on Tableau data analytics and Tableau server(admin)\nACADEMIC QUALIFICATIONS AND ACHIEVEMENTS\n\tDegree\n\tYear\n\tInstitution\n\tSpecialization\n\t%\n\n\tB-Tech\n\t2015\n\tHindustan college of Science and Technology\n\tElectronics and Communication Engineering\n\t80.84\n\n\t12th\n\t2011\n\tAir Force School, Agra\n\tPhysics, Chemistry and Mathematics, C++\n\t81\n\n\t10th\n\t2009\n\tAir Force School, Agra\n\tN.A.\n\t85.2\n\n\n\nSKILLS\n\tCertification\n\t· Tableau Server E0 Competency\n· Tableau desktop trainings (Prequel and sequel)\n· SQL (E0) Competency\n· ITIL Certification\n· Agile \n\n\tOS\n\t· Windows 7,8,10 and macOS Sierra(10.12.6) High Sierra(10.13.6)\n· Currently working on macOS Mojave(10.14.5)\n· Basic UNIX\n\n\tDatabase\n\tOracle 11g, Postgresql database\n\n\tTools\n\tTableau server (2019.3) and Tableau desktop (2019.3), Oracle 11g \n\n\n\n\n\n\nPROJECT WORK\n\t\n1. Project name: GBI (Global Business Intelligence) Tableau analytics\n              Client: Apple\n· Currently working in GBI Tableau project in Tableau analytics field.\n· Implemented attractive high performance dashboards as per requirement by apple client.\n· Used datasource and context  filters to prepare a healthy dashboard. Used extract filters for fast performance.\n· Worked on dashboard enhancement by adding columns and renaming them and applying the required filters.\n\n2. Project name: GBI (Global Business Intelligence) Tableau server\n              Client: Apple\n· Worked as a Tableau server admin.\n· Granted various access types to the users in Tableau such as Explorer, publisher, site admin and server admin.\n· Worked on providing project access and workbook  access. Provided customized workbook/view  access  such as (download, save, edit etc) with appropriate approvals from apple manager.\n· Completed Tableau server maintenance, Tableau OS Patching for multiple environments (Production, DEV, DR, Test).\n· Worked on Project  setup on  various environments of Tableau server for multiple users.\n· AD (Apple Directory) group provisioning setup: Created groups on tableau server and managed their sync up with Apple directory member count. Sync is managed by running Autosys jobs from Tableau portal. Also fixed provisioning failures.\n\n\n\n\n\n\n\n\nEXTRA CURRICULAR ACTIVITIES AND ACHIEVEMENTS\n· Participated in painting competition in my company\n· Organized and Participated in various events in technical and cultural eve during my college time.\n· Participated in painting competition in various events in class  XII.\n· Was elected as Miss Air Force First runner up in class XII.\n· Anchored entire farewell event for class XII in class XI. \n· Anchored in Gyan Jyoti fests in college.\n· Was a member of Music Quoir in school.\n\nPERSONAL DETAILS\nDate of Birth: 29/11/1993\nPassport: Available\nLanguages Known: English and Hindi","annotation":[{"label":["Skills"],"points":[{"start":2270,"end":2283,"text":"Tableau server"}]},{"label":["Skills"],"points":[{"start":2112,"end":2125,"text":"Tableau server"}]},{"label":["Total experience"],"points":[{"start":1805,"end":1913,"text":"Granted various access types to the users in Tableau such as Explorer, publisher, site admin and server admin"}]},{"label":["Skills"],"points":[{"start":1781,"end":1794,"text":"Tableau server"}]},{"label":["Skills"],"points":[{"start":1724,"end":1737,"text":"Tableau server"}]},{"label":["Total experience"],"points":[{"start":1454,"end":1563,"text":"Used datasource and context  filters to prepare a healthy dashboard. Used extract filters for fast performance"}]},{"label":["Total experience"],"points":[{"start":1365,"end":1449,"text":"Implemented attractive high performance dashboards as per requirement by apple client"}]},{"label":["Total experience"],"points":[{"start":1294,"end":1360,"text":"Currently working in GBI Tableau project in Tableau analytics field"}]},{"label":["Skills"],"points":[{"start":1161,"end":1170,"text":"Oracle 11g"}]},{"label":["Skills"],"points":[{"start":1135,"end":1149,"text":"Tableau desktop"}]},{"label":["Skills"],"points":[{"start":1107,"end":1120,"text":"Tableau server"}]},{"label":["Skills"],"points":[{"start":1066,"end":1075,"text":"Oracle 11g"}]},{"label":["Skills"],"points":[{"start":876,"end":878,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":827,"end":841,"text":"Tableau desktop"}]},{"label":["10 %"],"points":[{"start":733,"end":754,"text":"Air Force School, Agra"}]},{"label":["12 %"],"points":[{"start":651,"end":672,"text":"Air Force School, Agra"}]},{"label":["Highest degree"],"points":[{"start":543,"end":628,"text":"Hindustan college of Science and Technology\n\tElectronics and Communication Engineering"}]},{"label":["Highest degree"],"points":[{"start":529,"end":534,"text":"B-Tech"}]},{"label":["Skills"],"points":[{"start":418,"end":431,"text":"Tableau server"}]},{"label":["Experience in current company"],"points":[{"start":311,"end":342,"text":"Tata Consultancy Services, Noida"}]},{"label":["Total experience"],"points":[{"start":206,"end":262,"text":"Total 4 years 9 months experience in Software Development"}]},{"label":["Highest degree"],"points":[{"start":66,"end":71,"text":"B-Tech"}]},{"label":["Experience in current company"],"points":[{"start":33,"end":64,"text":"Tata Consultancy Services, Noida"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"Aparna Bhatia"}]}],"extras":null,"metadata":{"first_done_at":1624273142000,"last_updated_at":1624273142000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
{"content": "Pankaj Kumar \n \n\n \n \n\nObjective \n \nSeeking a challenging position in the area of Information Technology, which gives responsibility and opportunity               \n\nto grow with organization, where I can effectively contribute my skills as Software professional, possessing               \n\ncompetent Technical Skills. \n\n \nSummary \n \n• Total experience of 4+ years . \n\n• Having professional experience in Production & Development environments as Oracle PL/SQL Developer. \n\n \nSkill Summary \n  \n\n● Over 4 years of I.T. experience as Core ​Oracle SQL/PLSQL Developer. \n\n● Experience in developing complex packages, procedures,functions,triggers,etc. \n\n● Have hands on experience in Performance Tuning(SQL and PL/SQL) using EXPLAIN PLAN and \n\nTKProf. \n\n● Experience in using BULK COLLECT for better performance and easy data retrieval by reducing context \n\nswitching between SQL and PL/SQL engines. \n\n● Excellent verbal and written communication skills. \n\n● Efficient coding skills with proper error logging using PRAGMA. \n\n● Good Understanding of DBA concepts and Oracle Database Server. \n\n● Exceptionally good and experienced in handling problem and troubleshooting. \n\n● Additionally,I have sound knowledge of R and Python. \n\nWork Experience \n \n\nCurrent Employer :​ ​Currently working as a Oracle PL/SQL Developer for ​TCS,Andheri​,​Mumbai​(​Maharashtra​) \n\nTenure       : ​Jan, 2015– Till date \n\n Role                   : ​Oracle PL/SQL Developer \n\n Internal      :Ultimatix GESS \n\nEnvironment         :​ ​Oracle 11g,Oracle 12c \n\nResponsibilities: \n\n✓ Having over 4 years of experience as an ORACLE DEVELOPER. \n✓ Involved in Development,Unit Testing,Tuning, Production Deployment and Support activities. \n\n\n\nPankaj Kumar \n \n✓ Experience in developing advance PL/SQL packages, procedures, triggers, functions and efficient use of \n\nindexes and collections to implement the business logic. \n✓ Involved in the continuous new enhancements and fixing production issues. \n✓ Responsible for performance tuning activities. \n✓ Can write complex SQL queries for reporting needs. \n\n \n\n \nEducation \n \n• B Tech in Electronics and Telecommunication Engineering from KIIT University, Bhubaneswar. \n\n \nPersonal Information \n \nName :   Pankaj  Kumar \nFather’s Name :  Suresh Prasad \nSex :  Male \nMarital Status :  Unmarried \nCurrent Address              :  B-105,Swaroop Apartments,Mahakali Caves Road,Near Holy Spirit Hospital, \n                                                       Andheri(East),Mumbai(Maharashtra)-400093 \nPermanent Address :  Plot No. 461, Man-Mohan Co-Operative Colony,  \n                                                        B.S.City(Jharkhand)-827010 \nNationality :  INDIAN \n \n\nI am assuring you that the information declared above is true to the best of my knowledge. \n\n \n\nDate: 17-Mar-2019 \n\nPlace:            Mumbai  \n\n Pankaj Kumar","annotation":[{"label":["Name"],"points":[{"start":2830,"end":2841,"text":"Pankaj Kumar"}]},{"label":["Address"],"points":[{"start":2631,"end":2656,"text":"B.S.City(Jharkhand)-827010"}]},{"label":["Address"],"points":[{"start":2528,"end":2570,"text":"Plot No. 461, Man-Mohan Co-Operative Colony"}]},{"label":["Highest degree"],"points":[{"start":2088,"end":2176,"text":"B Tech in Electronics and Telecommunication Engineering from KIIT University, Bhubaneswar"}]},{"label":["Total experience"],"points":[{"start":1723,"end":1882,"text":"Experience in developing advance PL/SQL packages, procedures, triggers, functions and efficient use of \n\nindexes and collections to implement the business logic"}]},{"label":["Name"],"points":[{"start":1705,"end":1716,"text":"Pankaj Kumar"}]},{"label":["designation"],"points":[{"start":1420,"end":1442,"text":"Oracle PL/SQL Developer"}]},{"label":["Experience in current company"],"points":[{"start":1315,"end":1350,"text":"TCS,Andheri​,​Mumbai​(​Maharashtra​)"}]},{"label":["designation"],"points":[{"start":1286,"end":1308,"text":"Oracle PL/SQL Developer"}]},{"label":["Total experience"],"points":[{"start":896,"end":945,"text":" Excellent verbal and written communication skills"}]},{"label":["Total experience"],"points":[{"start":749,"end":847,"text":"Experience in using BULK COLLECT for better performance and easy data retrieval by reducing context"}]},{"label":["Total experience"],"points":[{"start":649,"end":742,"text":"Have hands on experience in Performance Tuning(SQL and PL/SQL) using EXPLAIN PLAN and \n\nTKProf"}]},{"label":["Total experience"],"points":[{"start":567,"end":642,"text":"Experience in developing complex packages, procedures,functions,triggers,etc"}]},{"label":["designation"],"points":[{"start":444,"end":466,"text":"Oracle PL/SQL Developer"}]},{"label":["Total experience"],"points":[{"start":369,"end":466,"text":"Having professional experience in Production & Development environments as Oracle PL/SQL Developer"}]},{"label":["Total experience"],"points":[{"start":35,"end":314,"text":"Seeking a challenging position in the area of Information Technology, which gives responsibility and opportunity               \n\nto grow with organization, where I can effectively contribute my skills as Software professional, possessing               \n\ncompetent Technical Skills"}]},{"label":["Name"],"points":[{"start":0,"end":11,"text":"Pankaj Kumar"}]}],"extras":null,"metadata":{"first_done_at":1624272806000,"last_updated_at":1624272806000,"sec_taken":0,"last_updated_by":"vikash.kumar@polestarllp.com","status":"done","evaluation":"NONE"}}
